
INTRO TO CLUSTERING

K-MEANS CLUSTERING
MORE ON K-MEANS 
LAB: K-MEANS

HIERARCHICAL CLUSTERING
MORE ON HIERARCHICAL CLUSTERING
LAB: HIERARCHICAL CLUSTERING

DBSCAN CLUSTERING
LAB: DBSCAN CLUSTERING

GRADED REVIEW QUESTIONS



K-Means Clustering plus Advantages & Disadvantages 
Hierarchical Clustering plus Advantages & Disadvantages 
Measuring the Distances Between Clusters - Single Linkage Clustering 
Measuring the Distances Between Clusters - Algorithms For Hierarchy Clustering 
Density-Based Clustering

**************************************************************************************
INTRO TO CLUSTERING
"CUSTOMER SEGMENTATION" : 
	Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data. Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics.  It allows a business to target specific groups of customers so as to more effectively allocate marketing resources. Example : For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe For a service. Knowing this information allows a business to devote more time and attention to retaining these customers. Another group might include customers from non-profit organizations, and so on.
	A general segmentation process is not usually feasible For large volumes of varied data. Therefore, you need an analytical approach to "deriving segments and groups from large data sets."

"CLUSTERING" 	: 
	One of the most adopted approaches that can be used For customer segmentation is clustering. Clustering can group data only “unsupervised,” based on the similarity of customers to each other. It will "partition your customers into mutually exclusive groups", For example, into 3 clusters.	
	The customers in each cluster are similar to each other demographically. Now we can create a profile For each group, considering the common characteristics of each cluster. 
	Example : 
		1. the first group is made up of AFFULUENT AND MIDDLE AGED customers. 
		2. The second is made up of YOUNG EDUCATED AND MIDDLE INCOME customers. 
		3. And the third group includes YOUNG AND LOW INCOME customers. 
		Finally, we can assign each individual in our dataset to one of these groups or segments of customers. 
		
		Now imagine that you cross-join this segmented dataset, with the dataset of the product or services that customers purchase from your company. This information would really help to understand and predict the differences in individual customers’ preferences and their buying behaviors across various products. Indeed, having this information would allow your company to develop highly personalized experiences For each segment.
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Segmentation.PNG"

"DEFINITION"	:
	Clustering means finding clusters in a dataset, unsupervised. 
	So, what is a cluster? 
		A cluster is group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to data points in other clusters. 
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_cluster.PNG"

"DIFFERNECE BETWEEN CLUSTERING AND CLASSIFICATION"	:
	CLASSIFICATION
		Classification algorithms predict categorical class labels. This means, assigning instances to pre-defined classes such as “Defaulted” or “Non-Defaulted.” For example, if an analyst wants to analyze customer data in order to know which customers might default on their payments, she uses a labeled dataset as training data, and uses classification approaches such as a decision tree, Support Vector Machines (or SVM), or, logistic regression to predict the default value For a new, or unknown customer. Generally speaking, "classification is a supervised learning where each training data instance belongs to a particular class."
	CLUSTERING
		In clustering, however, the data is unlabelled and the process is unsupervised. For example, we can use a clustering algorithm such as k-Means, to group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as age, education, and so on. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Classification.PNG"

"APPLICATIONS" 	:
		Retail/Marketing, Banking , Insurance
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_1.PNG"
		Publication, Medication, Biology
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_2.PNG"


"USES" 	:
	- Exploratory data analysis, 
	- summary generation or reducing the scale, 
	- outlier detection, especially to be used For fraud detection, or noise removal, 
	- finding duplicates in datasets, or, 
	- pre-processing step For either prediction, other data mining tasks, or, as part of a complex system.

"CLUSTERING ALGORITHMS"
1. Partitioned based Clustering - (produces sphere-like clusters - Medium and large sized data-sets)
	- Relatively efficient
	- K-means, k-median, Fuzzy c-Means

2. Heirarchical Clustering  - (For small sized data-sets)
	- Produces Trees of clusters
	- Agglomerative, Divisive

3. Density based Clustering
	- Produces arbitrary shaped cluster - (For noise based data-sets)
	- Eg. DBSCAN

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Algorithms.PNG"





**************************************************************************************
K-MEANS CLUSTERING
	- k-Means can group data only “unsupervised,” based on the similarity of customers to each other.
	- There are various types of clustering algorithms, such as partitioning, hierarchical, or density-based clustering. k-Means is a type of partitioning clustering, that is, it divides the data into k non-overlapping subsets (or clusters) without any cluster-internal structure, or labels. This means, it’s an unsupervised algorithm.
	- Objects within a cluster are very similar and objects across different clusters are very different or dissimilar. As you can see, For using k-Means, we have to find similar samples (for example, similar customers).

"“How can we find the similarity of samples in clustering?”"
	Though the objective of k-Means is to form clusters in such a way that similar samples go into a cluster, and dissimilar samples fall into different clusters, it can be shown that instead of a similarity metric, we can use dissimilarity metrics. In other words, conventionally, the distance of samples from each other is used to shape the clusters. So, we can say, 
		"K-MEANS TRIES TO MINIMIZE THE “INTRA-CLUSTER” DISTANCES AND MAXIMIZE THE “INTER-CLUSTER” DISTANCES."
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_inter_intra_cluster.PNG"

"“How do we measure how similar two customers are with regard to their demographics?”"
"“How we can calculate the dissimilarity or distance of two cases, such as two customers?”"
	Assume that we have two customers, we’ll call them customer 1 and 2. Let’s also assume that we have only one feature For each of these two customers, and that feature is Age. We can easily use a specific type of Minkowski distance to calculate the distance of these two customers. Indeed, it is the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 power 2, which is 4. What about if we have more than one feature, For example Age and Income? For example, if we have income and age For each customer, we can still use the same formula, but this time in a 2-dimensional space. Also, we can use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose, but it is highly dependent on data type and also the domain that clustering is done For it. For example, you may use Euclidean distance, cosine similarity, average distance, and so on. Indeed, the similarity measure highly controls how the clusters are formed, so it is recommended to understand the domain knowledge of your dataset, and data type of features, and then choose the meaningful distance measurement. 

WORKING		:

1. Initialize k
	The key concept of the k-Means algorithm is that it randomly picks a center point For each cluster. It means, we must initialize k,which represents "number of clusters" There are two approaches to choose these centroids:
			a.  We can randomly choose 3 observations out of the dataset and use these observations as the initial means. 
			b.  We can create 3 random points as centroids of the clusters

2. Distance Calculation
	For this purpose, we have to calculate the distance of each data point (or in our case, each customer) from the centroid points.
	Therefore, you will form a matrix where each row represents the distance of a customer from each centroid. It is called the "distance-matrix."

3. Assign each point to the closest centroid. (Assign each customer to the closest center). - distance-matrix
	@The main objective of k_Means clustering Is to minimize the distance of data points From the centroid of its cluster And maximize the distance From other cluster centroids
	Find the closest centroid to each data point. We can use the distance-matrix to find the nearest centroid to data points. Finding the closest centroids For each data point, we assign each data point to that cluster. In other words, all the customers will fall to a cluster, based on their distance from centroids. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_distance_matrix.PNG"
	error is the total distance of each point from its centroid. It can be shown as within-cluster sum of squares error. 	
		SSE = Sum of squared distances between each point and its centroid.

		SSE =  Σ ( xᵢ - Cᵢ )²
		 Intuitively, we try to reduce this error. It means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized.
 "How we can turn it into better clusters, with less error?"
4. Compute the new centroid For each cluster
	We move centroids. Each cluster center will be updated to be the" mean For data points" in its cluster. Indeed, each centroid moves according to their cluster members. In other words, the centroid of each of the 3 clusters becomes the new mean. 
	For example, if Point A coordination is 7.4 and 3.6, and Point B features are 7.8 and 3.8, the new centroid of this cluster with 2 points, would be the average of them, which is 7.6 and 3.7.

5. Repeat 2-4 untill there are no more changes
	Now we have new centroids. As you can guess, once again, we will have to calculate the distance of all points from the new centroids. The points are re-clustered and the centroids move again. This continues until the centroids no longer move. Please note that whenever a centroid moves, each point’s distance to the centroid needs to be measured again. Yes, k-Means is an iterative algorithm, and we have to repeat steps 2 to 4 until the algorithm converges. In each iteration, it will move the centroids, calculate the distances from new centroids, and assign the data points to the nearest centroid. It results in the clusters with minimum error, or the most dense clusters. However, as it is a heuristic algorithm, there is no guarantee that it will converge to the Global optimum, and the result may depend on the initial clusters. It means this algorithm is guaranteed to converge to a result, but the result may be a local optimum (i.e. not necessarily the best possible outcome). 



MORE ON K-MEANS 
Algorithm
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_Algo.PNG"


ACCURACY 	:
	 "How can we evaluate the 'goodness' of the clusters formed by k-Means?" In other words, 
	 "How do we calculate the accuracy of k-Means clustering?"

	 1. External approach 	
	 	Compare the clusters with groud truth, if available- (Mostly un-available since un-supervised data or un-labeled data)
	 
	 2. Internal approach
	 	This value is the average distance between data points within a cluster. Also, average of the distances of data points from their cluster centroids can be used as a metric of error For the clustering algorithm.










LAB: K-MEANS

**************************************************************************************
HIERARCHICAL CLUSTERING
MORE ON HIERARCHICAL CLUSTERING
LAB: HIERARCHICAL CLUSTERING

**************************************************************************************
DBSCAN CLUSTERING
LAB: DBSCAN CLUSTERING

**************************************************************************************
GRADED REVIEW QUESTIONS
