
INTRO TO CLUSTERING

K-MEANS CLUSTERING
MORE ON K-MEANS 
LAB: K-MEANS

HIERARCHICAL CLUSTERING
MORE ON HIERARCHICAL CLUSTERING
LAB: HIERARCHICAL CLUSTERING

DBSCAN CLUSTERING
LAB: DBSCAN CLUSTERING

GRADED REVIEW QUESTIONS



K-Means Clustering plus Advantages & Disadvantages 
Hierarchical Clustering plus Advantages & Disadvantages 
Measuring the Distances Between Clusters - Single Linkage Clustering 
Measuring the Distances Between Clusters - Algorithms For Hierarchy Clustering 
Density-Based Clustering

**************************************************************************************
INTRO TO CLUSTERING
"CUSTOMER SEGMENTATION" : 
	Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data. Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics.  It allows a business to target specific groups of customers so as to more effectively allocate marketing resources. Example : For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe For a service. Knowing this information allows a business to devote more time and attention to retaining these customers. Another group might include customers from non-profit organizations, and so on.
	A general segmentation process is not usually feasible For large volumes of varied data. Therefore, you need an analytical approach to "deriving segments and groups from large data sets."

"CLUSTERING" 	: 
	One of the most adopted approaches that can be used For customer segmentation is clustering. Clustering can group data only “unsupervised,” based on the similarity of customers to each other. It will "partition your customers into mutually exclusive groups", For example, into 3 clusters.	
	The customers in each cluster are similar to each other demographically. Now we can create a profile For each group, considering the common characteristics of each cluster. 
	Example : 
		1. the first group is made up of AFFULUENT AND MIDDLE AGED customers. 
		2. The second is made up of YOUNG EDUCATED AND MIDDLE INCOME customers. 
		3. And the third group includes YOUNG AND LOW INCOME customers. 
		Finally, we can assign each individual in our dataset to one of these groups or segments of customers. 
		
		Now imagine that you cross-join this segmented dataset, with the dataset of the product or services that customers purchase from your company. This information would really help to understand and predict the differences in individual customers’ preferences and their buying behaviors across various products. Indeed, having this information would allow your company to develop highly personalized experiences For each segment.
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Segmentation.PNG"

"DEFINITION"	:
	Clustering means finding clusters in a dataset, unsupervised. 
	So, what is a cluster? 
		A cluster is group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to data points in other clusters. 
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_cluster.PNG"

"DIFFERNECE BETWEEN CLUSTERING AND CLASSIFICATION"	:
	CLASSIFICATION
		Classification algorithms predict categorical class labels. This means, assigning instances to pre-defined classes such as “Defaulted” or “Non-Defaulted.” For example, if an analyst wants to analyze customer data in order to know which customers might default on their payments, she uses a labeled dataset as training data, and uses classification approaches such as a decision tree, Support Vector Machines (or SVM), or, logistic regression to predict the default value For a new, or unknown customer. Generally speaking, "classification is a supervised learning where each training data instance belongs to a particular class."
	CLUSTERING
		In clustering, however, the data is unlabelled and the process is unsupervised. For example, we can use a clustering algorithm such as k-Means, to group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as age, education, and so on. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Classification.PNG"

"APPLICATIONS" 	:
		Retail/Marketing, Banking , Insurance
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_1.PNG"
		Publication, Medication, Biology
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_2.PNG"


"USES" 	:
	- Exploratory data analysis, 
	- summary generation or reducing the scale, 
	- outlier detection, especially to be used For fraud detection, or noise removal, 
	- finding duplicates in datasets, or, 
	- pre-processing step For either prediction, other data mining tasks, or, as part of a complex system.

"CLUSTERING ALGORITHMS"
1. Partitioned based Clustering - (produces sphere-like clusters - Medium and large sized data-sets)
	- Relatively efficient
	- K-means, k-median, Fuzzy c-Means

2. Heirarchical Clustering  - (For small sized data-sets)
	- Produces Trees of clusters
	- Agglomerative, Divisive

3. Density based Clustering
	- Produces arbitrary shaped cluster - (For noise based data-sets)
	- Eg. DBSCAN

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Algorithms.PNG"





**************************************************************************************
K-MEANS CLUSTERING
	- k-Means can group data only “unsupervised,” based on the similarity of customers to each other.
	- There are various types of clustering algorithms, such as partitioning, hierarchical, or density-based clustering. k-Means is a type of partitioning clustering, that is, it divides the data into k non-overlapping subsets (or clusters) without any cluster-internal structure, or labels. This means, it’s an unsupervised algorithm.
	- Objects within a cluster are very similar and objects across different clusters are very different or dissimilar. As you can see, For using k-Means, we have to find similar samples (for example, similar customers).

"“How can we find the similarity of samples in clustering?”"
	Though the objective of k-Means is to form clusters in such a way that similar samples go into a cluster, and dissimilar samples fall into different clusters, it can be shown that instead of a similarity metric, we can use dissimilarity metrics. In other words, conventionally, the distance of samples from each other is used to shape the clusters. So, we can say, 
		"K-MEANS TRIES TO MINIMIZE THE “INTRA-CLUSTER” DISTANCES AND MAXIMIZE THE “INTER-CLUSTER” DISTANCES."
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_inter_intra_cluster.PNG"

"“How do we measure how similar two customers are with regard to their demographics?”"
"“How we can calculate the dissimilarity or distance of two cases, such as two customers?”"
	Assume that we have two customers, we’ll call them customer 1 and 2. Let’s also assume that we have only one feature For each of these two customers, and that feature is Age. We can easily use a specific type of Minkowski distance to calculate the distance of these two customers. Indeed, it is the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 power 2, which is 4. What about if we have more than one feature, For example Age and Income? For example, if we have income and age For each customer, we can still use the same formula, but this time in a 2-dimensional space. Also, we can use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose, but it is highly dependent on data type and also the domain that clustering is done For it. For example, you may use Euclidean distance, cosine similarity, average distance, and so on. Indeed, the similarity measure highly controls how the clusters are formed, so it is recommended to understand the domain knowledge of your dataset, and data type of features, and then choose the meaningful distance measurement. 

WORKING		:

1. Initialize k
	The key concept of the k-Means algorithm is that it randomly picks a center point For each cluster. It means, we must initialize k,which represents "number of clusters" There are two approaches to choose these centroids:
			a.  We can randomly choose 3 observations out of the dataset and use these observations as the initial means. 
			b.  We can create 3 random points as centroids of the clusters

2. Distance Calculation
	For this purpose, we have to calculate the distance of each data point (or in our case, each customer) from the centroid points.
	Therefore, you will form a matrix where each row represents the distance of a customer from each centroid. It is called the "distance-matrix."

3. Assign each point to the closest centroid. (Assign each customer to the closest center). - distance-matrix
	@The main objective of k_Means clustering Is to minimize the distance of data points From the centroid of its cluster And maximize the distance From other cluster centroids
	Find the closest centroid to each data point. We can use the distance-matrix to find the nearest centroid to data points. Finding the closest centroids For each data point, we assign each data point to that cluster. In other words, all the customers will fall to a cluster, based on their distance from centroids. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_distance_matrix.PNG"
	error is the total distance of each point from its centroid. It can be shown as within-cluster sum of squares error. 	
		SSE = Sum of squared distances between each point and its centroid.

		SSE =  Σ ( xᵢ - Cᵢ )²
		 Intuitively, we try to reduce this error. It means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized.
 "How we can turn it into better clusters, with less error?"
4. Compute the new centroid For each cluster
	We move centroids. Each cluster center will be updated to be the" mean For data points" in its cluster. Indeed, each centroid moves according to their cluster members. In other words, the centroid of each of the 3 clusters becomes the new mean. 
	For example, if Point A coordination is 7.4 and 3.6, and Point B features are 7.8 and 3.8, the new centroid of this cluster with 2 points, would be the average of them, which is 7.6 and 3.7.

5. Repeat 2-4 untill there are no more changes
	Now we have new centroids. As you can guess, once again, we will have to calculate the distance of all points from the new centroids. The points are re-clustered and the centroids move again. This continues until the centroids no longer move. Please note that whenever a centroid moves, each point’s distance to the centroid needs to be measured again. Yes, k-Means is an iterative algorithm, and we have to repeat steps 2 to 4 until the algorithm converges. In each iteration, it will move the centroids, calculate the distances from new centroids, and assign the data points to the nearest centroid. It results in the clusters with minimum error, or the most dense clusters. However, as it is a heuristic algorithm, there is no guarantee that it will converge to the Global optimum, and the result may depend on the initial clusters. It means this algorithm is guaranteed to converge to a result, but the result may be a local optimum (i.e. not necessarily the best possible outcome). 



MORE ON K-MEANS 
Algorithm
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_Algo.PNG"


ACCURACY 	:
	 "How can we evaluate the 'goodness' of the clusters formed by k-Means?" In other words, 
	 "How do we calculate the accuracy of k-Means clustering?"

	 1. External approach 	
	 	Compare the clusters with groud truth, if available- (Mostly un-available since un-supervised data or un-labeled data)
	 
	 2. Internal approach
	 	This value is the "AVERAGE DISTANCE BETWEEN DATA POINTS WITHIN A CLUSTER". Also, average of the distances of data points from their cluster centroids can be used as a metric of error For the clustering algorithm.

CHOOSING K  :
	 The correct choice of k is often ambiguous, because it’s very dependent on the shape and scale of the distribution of points in a data set. There are some approaches to address this problem, but one of the techniques that is commonly used, is to run the clustering across the different values of K, and looking at a metric of accuracy For clustering. This metric can be "“MEAN DISTANCE BETWEEN DATA POINTS AND THEIR CLUSTER CENTROID,” " which indicate how dense our clusters are, or to what extend we minimized the error of clustering. 
	 But the problem is that with increasing the number of clusters, the distance of centroids to data points will always reduce. This means, "increasing K will always decrease the “error.”" So, the value of the metric as a function of K is plotted and the "elbow point" is determined, where the rate of decrease sharply shifts. It is the right K For clustering. This method is called the "“ELBOW” METHOD."
	 "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_Choosing_K_Elbow_method.PNG"


SUMMARY :
	k-Means is a partitioned-based clustering, which is:
		a. Relatively efficient on medium and large sized datasets;
		b. Produces sphere-like clusters, because the clusters are shaped around the centroids;
		c. Its drawback is that we should pre-specify the number of clusters-(k), and this is not an easy task.




LAB: K-MEANS
from sklearn.cluster import KMeans 

Some real-world applications of k-means:
	- Customer segmentation
	- Understand what the visitors of a website are trying to accomplish
	- Pattern recognition
	- Machine learning
	- Data compression

In this notebook we practice k-means clustering with 2 examples.
	- k-means on a random generated dataset	
	- Using k-means For customer segmentation

\k-Means on a randomly generated dataset
from sklearn.datasets.samples_generator import make_blobs 
We will be making random clusters of points by using the make_blobs class. The make_blobs class can take in many inputs, but we will be using these specific ones. 
"Input"
	n_samples: The total number of points equally divided among clusters.
		Value will be: 5000
	centers: The number of centers to generate, or the fixed center locations.
		Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]]
	cluster_std: The standard deviation of the clusters.
		Value will be: 0.9
"Output"
	X: Array of shape [n_samples, n_features]. (Feature Matrix)
		The generated samples.
	y: Array of shape [n_samples]. (Response Vector)
		The integer labels For cluster membership of each sample.


X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)
X, y
(array([[-0.91724214, -0.65544337],
        [-1.18588643, -0.82839009],
        [-0.14874474,  0.76529566],
        ...,
        [ 1.32068954,  0.97290081],
        [ 0.84875774, -3.40991974],
        [ 4.73720907,  3.05726509]]), array([1, 1, 3, ..., 3, 2, 0]))


plt.scatter(X[:, 0], X[:, 1], marker='.')

\Setting up K-Means

init: Initialization method of the centroids.
	Value will be: "k-means++"
	k-means++: Selects initial cluster centers For k-mean clustering in a smart way to speed up convergence.
n_clusters: The number of clusters to form as well as the number of centroids to generate.
	Value will be: 4 (since we have 4 centers)
n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.
	Value will be: 12

k_means = KMeans(init = "k-means++", n_clusters = 4, n_init = 12)
k_means.fit(X)
>>>
KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=4, n_init=12, n_jobs=None, precompute_distances='auto',
    random_state=None, tol=0.0001, verbose=0)

k_means_labels = k_means.labels_
>>>
array([0, 3, 3, ..., 1, 0, 0], dtype=int32)

k_means_cluster_centers = k_means.cluster_centers_
>>>
array([[-2.03743147, -0.99782524],
       [ 3.97334234,  3.98758687],
       [ 0.96900523,  0.98370298],
       [ 1.99741008, -3.01666822]])


\Creating the Visual Plot - k-means
# Initialize the plot with the specified dimensions.
fig = plt.figure(figsize=(6, 4))

# Colors uses a color map, which will produce an array of colors based on
# the number of labels there are. We use set(k_means_labels) to get the
# unique labels.
colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))

# Create a plot
ax = fig.add_subplot(1, 1, 1)

# For loop that plots the data points and centroids.
# k will range from 0-3, which will match the possible clusters that each
# data point is in.
for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):

    # Create a list of all data points, where the data poitns that are 
    # in the cluster (ex. cluster 0) are labeled as true, else they are
    # labeled as false.
    my_members = (k_means_labels == k)
    
    # Define the centroid, or cluster center.
    cluster_center = k_means_cluster_centers[k]
    
    # Plots the datapoints with color col.
    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')
    
    # Plots the centroids with specified color, but with a darker outline
    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)

# Title of the plot
ax.set_title('KMeans')

# Remove x-axis ticks
ax.set_xticks(())

# Remove y-axis ticks
ax.set_yticks(())

# Show the plot
plt.show()



\Customer Segmentation with K-Means

import pandas as pd
cust_df = pd.read_csv("Cust_Segmentation.csv")
cust_df.head()
>>>
	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	Address	DebtIncomeRatio
0	1		 	41	2	6				19	   	0.124		1.073		0.0			NBA001	6.3
1	2		 	47	1	26				100	   	4.582		8.218		0.0			NBA021	12.8
2	3		 	33	2	10				57	   	6.111		5.802		1.0			NBA013	20.9
3	4		 	29	2	4				19	   	0.681		0.516		0.0			NBA009	6.3
4	5		 	47	1	31				253	   	9.308		8.908		0.0			NBA008	7.2

@PRE_PROCESSING
#As you can see, Address in this dataset is a categorical variable. k-means algorithm isn`t directly applicable to categorical variables because Euclidean distance function isn`t really meaningful For discrete variables. So, lets drop this feature and run clustering.
df = cust_df.drop('Address', axis=1)
df.head()
	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	DebtIncomeRatio
0	1		 	41	2	6				19	   	0.124		1.073		0.0			6.3
1	2		 	47	1	26				100	   	4.582		8.218		0.0			12.8
2	3		 	33	2	10				57	   	6.111		5.802		1.0			20.9
3	4		 	29	2	4				19	   	0.681		0.516		0.0			6.3
4	5		 	47	1	31				253	   	9.308		8.908		0.0			7.2



@NORMALIZING OVER THE STANDARD DEVIATION
# WHY ? Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use standardScaler() to normalize our dataset.
from sklearn.preprocessing import StandardScaler
X = df.values[:,1:]
X = np.nan_to_num(X)
Clus_dataSet = StandardScaler().fit_transform(X)
>>>
array([[ 0.74291541,  0.31212243, -0.37878978, ..., -0.59048916,
        -0.52379654, -0.57652509],
       [ 1.48949049, -0.76634938,  2.5737211 , ...,  1.51296181,
        -0.52379654,  0.39138677],
       [-0.25251804,  0.31212243,  0.2117124 , ...,  0.80170393,
         1.90913822,  1.59755385],
       ...,
       [-1.24795149,  2.46906604, -1.26454304, ...,  0.03863257,
         1.90913822,  3.45892281],
       [-0.37694723, -0.76634938,  0.50696349, ..., -0.70147601,
        -0.52379654, -1.08281745],
       [ 2.1116364 , -0.76634938,  1.09746566, ...,  0.16463355,
        -0.52379654, -0.2340332 ]])


@MODELING
#In our example (if we didn't have access to the k-means algorithm), it would be the same as guessing that each customer group would have certain age, income, education, etc, with multiple tests and experiments. However, using the K-means clustering we can do all this process much easier.
clusterNum = 3
k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means.fit(X)
labels = k_means.labels_
print(labels)
>>>
["0 2 0 0 1" 2 0 2 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 2 2 2 0 0 2 0 2 0 0 0 0 0 0
 0 0 2 0 2 0 1 0 2 0 0 0 2 2 0 0 2 2 0 0 0 2 0 2 0 2 2 0 0 2 0 0 0 2 2 2 0
 0 0 0 0 2 0 2 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 2 0
 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 2 0 2 0
 0 0 0 0 0 0 2 0 2 2 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 2 0
 0 0 0 0 2 0 0 2 0 2 0 0 2 1 0 2 0 0 0 0 0 0 1 2 0 0 0 0 2 0 0 2 2 0 2 0 2
 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 2 0 0 0 0
 0 0 2 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 2 0 2 0 2 2 0 0 0 0 0 0
 0 0 0 2 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 2 0 2 2 0
 0 0 0 0 2 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 0 2 0 0 0 1 0 0 0 2 0 2 2 2 0
 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0
 0 2 0 0 2 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 1
 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 2 0 1 0 0 0 0 2 0 2 2 2 0 0 2 2 0 0 0 0 0 0
 0 2 0 0 0 0 2 0 0 0 2 0 2 0 0 0 2 0 0 0 0 2 2 0 0 0 0 2 0 0 0 0 2 0 0 0 0
 0 2 2 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 1 0 1 0
 0 1 0 0 0 0 0 0 0 0 0 2 0 2 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 2
 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2
 2 0 0 2 0 2 0 0 2 0 2 0 0 1 0 2 0 2 0 0 0 0 0 2 2 0 0 0 0 2 0 0 0 2 2 0 0
 2 0 0 0 2 0 1 0 0 2 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0
 0 0 2 0 0 2 0 2 0 2 2 0 0 0 2 0 2 0 0 0 0 0 2 0 0 0 0 2 2 0 0 2 2 0 0 0 0
 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 0 2 0 2 2 0 0 2 0 0 0 0 0 2 2
 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2]

@INSIGHTS

 df["Clus_km"] = labels
 	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	DebtIncomeRatio \Clus_km
0	1		 	41	2	6				19	   	0.124		1.073		0.0			6.3				0
1	2		 	47	1	26				100	   	4.582		8.218		0.0			12.8			2
2	3		 	33	2	10				57	   	6.111		5.802		1.0			20.9			0
3	4		 	29	2	4				19	   	0.681		0.516		0.0			6.3				0
4	5		 	47	1	31				253	   	9.308		8.908		0.0			7.2				1


# check the centroid values by averaging the features in each cluster.
df.groupby('Clus_km').mean()
>>>
		Customer Id	Age			Edu			Years Employed	Income		Card Debt	Other Debt	Defaulted	DebtIncomeRatio
Clus_km									
0		432.468413	32.964561	1.614792	6.374422		31.164869	1.032541	2.104133	0.285185	10.094761
1		410.166667	45.388889	2.666667	19.555556		227.166667	5.678444	10.907167	0.285714	7.322222
2		402.295082	41.333333	1.956284	15.256831		83.928962	3.103639	5.765279	0.171233	10.724590




X[:, 0] = Age
X[:, 1] = Edu
X[:, 3] = Income
# IN 2d
# distribution of customers based on their age and income:
area = np.pi * ( X[:, 1])**2  # Edu is the size of scatter circle
plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(np.float), alpha=0.5)
plt.xlabel('Age', fontsize=18)
plt.ylabel('Income', fontsize=16)

plt.show()

# IN 3d
from mpl_toolkits.mplot3d import Axes3D 
fig = plt.figure(1, figsize=(8, 6))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()
# plt.ylabel('Age', fontsize=18)
# plt.xlabel('Income', fontsize=16)
# plt.zlabel('Education', fontsize=16)
ax.set_xlabel('Education')
ax.set_ylabel('Age')
ax.set_zlabel('Income')

ax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(np.float))

k-means will partition your customers into mutually exclusive groups, For example, into 3 clusters. The customers in each cluster are similar to each other demographically. Now we can create a profile For each group, considering the common characteristics of each cluster. For example, the 3 clusters can be:

AFFLUENT, EDUCATED AND OLD AGED
MIDDLE AGED AND MIDDLE INCOME
YOUNG AND LOW INCOME

**************************************************************************************
HIERARCHICAL CLUSTERING
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_dendogram_breed_full.jpg"
	This diagram, shows hierarchical clustering of these animals based on the similarity in their genetic data.


Hierarchical clustering algorithms build a hierarchy of clusters where each node is a cluster consisting of the clusters of its daughter nodes. 

Strategies For hierarchical clustering generally fall into two types: 
	1. Divisive
		Divisive is top-down, so you start with all observations in a large cluster and break it down into smaller pieces. Think about divisive as "dividing" the cluster.
	2. Agglomerative.
		Agglomerative is the opposite of divisive, so it is bottom-up, where each observation starts in its own cluster and pairs of clusters are merged together as they move up the hierarchy. Agglomeration means to amass or collect things, which is exactly what this does with the cluster.


Agglomerative clustering.
This method builds the hierarchy from the individual elements by progressively merging clusters. 

EXAMPLE : let’s say we want to cluster 6 cities in Canada based on their distances from one another.
They are: Toronto, Ottawa, Vancouver, Montreal, Winnipeg, and Edmonton. We construct a distance matrix at this stage, where the numbers in the row i column j is the distance between the i and j cities. In fact, this table shows the distances between each pair of cities. The algorithm is started by assigning each city to its own cluster. So, if we have 6 cities, we have 6 clusters, each containing just one city. Let’s note each city by showing the first two characters of its name. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram.PNG"
	The first step is to determine which cities to merge into a cluster. 
		Usually, we want to take the two closest clusters according to the chosen distance. Looking at the distance matrix, Montreal and Ottawa are the closest clusters. So, we make a cluster out of them. Please notice that we just use a simple 1-dimentional distance feature here, but our object can be multi-dimensional, and distance measurement can be either Euclidean, Pearson, average distance, or many others, depending on data type and domain knowledge. Anyhow, we have to merge these two closest cities in the distance matrix as well. So, rows and columns are merged as the cluster is constructed. As you can see in the distance matrix, rows and columns related to Montreal and Ottawa cities are merged as the cluster is constructed. Then, the distances from all cities to this new merged cluster get updated. But how? For example, how do we calculate the distance from Winnipeg to the Ottawa-Montreal cluster? Well, there are different approaches, but let’s assume, For example, we just select the distance from the centre of the Ottawa-Montreal cluster to Winnipeg. Updating the distance matrix, we now have one less cluster. Next, we look For the closest clusters once again. In this case, Ottawa-Montreal and Toronto are the closest ones, which creates another cluster. In the next step, the closest distance is between the Vancouver cluster and the Edmonton cluster. Forming a new cluster, their data in the matrix table gets updated. Essentially, the rows and columns are merged as the clusters are merged and the distance updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. In the same way, agglomerative algorithm proceeds by merging clusters. And we repeat it until all clusters are merged and the tree becomes completed. It means, until all cities are clustered into a single cluster of size 6. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram_1.PNG"
	- Hierarchical clustering is typically visualized as a dendrogram as shown on this slide. 
	- Each merge is represented by a horizontal line. 
	- The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. 
	- By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. 
	- Essentially, Hierarchical clustering does not require a pre-specified number of clusters.

	- However, in some applications we want a partition of disjoint clusters just as in flat clustering. 
		In those cases, the hierarchy needs to be cut at some point. 
		For example here, cutting in a specific level of similarity, we create 3 clusters of similar cities.
   "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram_2.PNG"




MORE ON HIERARCHICAL CLUSTERING
Agglomerative algorithm for Hierarchical Clustering. Remember that Agglomerative clustering is a bottom-up approach.
Let’s say our dataset has n data points. 
	1. "Create n clusters, one for each data point". Then each point is assigned as a cluster.
	2. "Compute the distance/proximity matrix", which will be an n by n table.
	3. REPEAT : (until the specified cluster number is reached, or until there is only one cluster left.)
			i.  "MERGE the two nearest clusters". (Distances are computed already in the proximity matrix.)
			ii. "UPDATE the proximity matrix" with the new values.
	4. UNTIL only single cluster remains 

So, in the proximity matrix, we have to measure the distances between clusters, and also merge the clusters that are “nearest.” So, the key operation is the computation of the proximity between the clusters with one point, and also clusters with multiple data points.

A. "How do we measure the distances between these clusters and How do we define the ‘nearest’ among clusters?"
B. "Which points do we use?"

First, let’s see how to calculate the distance between 2 clusters with 1 point each. Let’s assume that we have a dataset of patients, and we want to cluster them using hierarchy clustering. So, our data points are patients, with a feature set of 3 dimensions. For example, Age, Body Mass Index (or BMI), and Blood Pressure. We can use different distance measurements to calculate the proximity matrix. For instance, Euclidean distance. So, if we have a dataset of n patients, we can build an n by n dissimilarly-distance matrix. It will give us the distance of clusters with 1 data point. 
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_SimilarityMatrix.PNG"

However, as mentioned, we merge clusters in Agglomerative clustering. Now, the question is, "How can we calculate the distance between clusters when there are multiple patients in each cluster?"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_SimilarityMatrix_betweenClusters.PNG"	


	1. Single-linkage clustering
		Minimum distance between clusters

	2. Complete-linkage clustering
		Maximum distance between clusters

	3. Average-linkage clustering
		Average distance between clusters

	4. Centroid-linkage clustering
		Distance between cluster centroids
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_Clustering_linkage.PNG"











LAB: HIERARCHICAL CLUSTERING

**************************************************************************************
DBSCAN CLUSTERING

LAB: DBSCAN CLUSTERING

**************************************************************************************
GRADED REVIEW QUESTIONS
