
INTRO TO CLUSTERING

K-MEANS CLUSTERING
MORE ON K-MEANS 
LAB: K-MEANS

HIERARCHICAL CLUSTERING
MORE ON HIERARCHICAL CLUSTERING
LAB: HIERARCHICAL CLUSTERING

DBSCAN CLUSTERING
LAB: DBSCAN CLUSTERING

GRADED REVIEW QUESTIONS



K-Means Clustering plus Advantages & Disadvantages 
Hierarchical Clustering plus Advantages & Disadvantages 
Measuring the Distances Between Clusters - Single Linkage Clustering 
Measuring the Distances Between Clusters - Algorithms For Hierarchy Clustering 
Density-Based Clustering

**************************************************************************************
INTRO TO CLUSTERING
"CUSTOMER SEGMENTATION" : 
	Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data. Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics.  It allows a business to target specific groups of customers so as to more effectively allocate marketing resources. Example : For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe For a service. Knowing this information allows a business to devote more time and attention to retaining these customers. Another group might include customers from non-profit organizations, and so on.
	A general segmentation process is not usually feasible For large volumes of varied data. Therefore, you need an analytical approach to "deriving segments and groups from large data sets."

"CLUSTERING" 	: 
	One of the most adopted approaches that can be used For customer segmentation is clustering. Clustering can group data only â€œunsupervised,â€ based on the similarity of customers to each other. It will "partition your customers into mutually exclusive groups", For example, into 3 clusters.	
	The customers in each cluster are similar to each other demographically. Now we can create a profile For each group, considering the common characteristics of each cluster. 
	Example : 
		1. the first group is made up of AFFULUENT AND MIDDLE AGED customers. 
		2. The second is made up of YOUNG EDUCATED AND MIDDLE INCOME customers. 
		3. And the third group includes YOUNG AND LOW INCOME customers. 
		Finally, we can assign each individual in our dataset to one of these groups or segments of customers. 
		
		Now imagine that you cross-join this segmented dataset, with the dataset of the product or services that customers purchase from your company. This information would really help to understand and predict the differences in individual customersâ€™ preferences and their buying behaviors across various products. Indeed, having this information would allow your company to develop highly personalized experiences For each segment.
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Segmentation.PNG"

"DEFINITION"	:
	Clustering means finding clusters in a dataset, unsupervised. 
	So, what is a cluster? 
		A cluster is group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to data points in other clusters. 
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_cluster.PNG"

"DIFFERNECE BETWEEN CLUSTERING AND CLASSIFICATION"	:
	CLASSIFICATION
		Classification algorithms predict categorical class labels. This means, assigning instances to pre-defined classes such as â€œDefaultedâ€ or â€œNon-Defaulted.â€ For example, if an analyst wants to analyze customer data in order to know which customers might default on their payments, she uses a labeled dataset as training data, and uses classification approaches such as a decision tree, Support Vector Machines (or SVM), or, logistic regression to predict the default value For a new, or unknown customer. Generally speaking, "classification is a supervised learning where each training data instance belongs to a particular class."
	CLUSTERING
		In clustering, however, the data is unlabelled and the process is unsupervised. For example, we can use a clustering algorithm such as k-Means, to group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as age, education, and so on. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Classification.PNG"

"APPLICATIONS" 	:
		Retail/Marketing, Banking , Insurance
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_1.PNG"
		Publication, Medication, Biology
			"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_application_2.PNG"


"USES" 	:
	- Exploratory data analysis, 
	- summary generation or reducing the scale, 
	- outlier detection, especially to be used For fraud detection, or noise removal, 
	- finding duplicates in datasets, or, 
	- pre-processing step For either prediction, other data mining tasks, or, as part of a complex system.

"CLUSTERING ALGORITHMS"
1. Partitioned based Clustering - (produces sphere-like clusters - Medium and large sized data-sets)
	- Relatively efficient
	- K-means, k-median, Fuzzy c-Means

2. Heirarchical Clustering  - (For small sized data-sets)
	- Produces Trees of clusters
	- Agglomerative, Divisive

3. Density based Clustering
	- Produces arbitrary shaped cluster - (For noise based data-sets)
	- Eg. DBSCAN

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Clustering_Algorithms.PNG"





**************************************************************************************
K-MEANS CLUSTERING
	- k-Means can group data only â€œunsupervised,â€ based on the similarity of customers to each other.
	- There are various types of clustering algorithms, such as partitioning, hierarchical, or density-based clustering. k-Means is a type of partitioning clustering, that is, it divides the data into k non-overlapping subsets (or clusters) without any cluster-internal structure, or labels. This means, itâ€™s an unsupervised algorithm.
	- Objects within a cluster are very similar and objects across different clusters are very different or dissimilar. As you can see, For using k-Means, we have to find similar samples (for example, similar customers).

"â€œHow can we find the similarity of samples in clustering?â€"
	Though the objective of k-Means is to form clusters in such a way that similar samples go into a cluster, and dissimilar samples fall into different clusters, it can be shown that instead of a similarity metric, we can use dissimilarity metrics. In other words, conventionally, the distance of samples from each other is used to shape the clusters. So, we can say, 
		"K-MEANS TRIES TO MINIMIZE THE â€œINTRA-CLUSTERâ€ DISTANCES AND MAXIMIZE THE â€œINTER-CLUSTERâ€ DISTANCES."
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_inter_intra_cluster.PNG"

"â€œHow do we measure how similar two customers are with regard to their demographics?â€"
"â€œHow we can calculate the dissimilarity or distance of two cases, such as two customers?â€"
	Assume that we have two customers, weâ€™ll call them customer 1 and 2. Letâ€™s also assume that we have only one feature For each of these two customers, and that feature is Age. We can easily use a specific type of Minkowski distance to calculate the distance of these two customers. Indeed, it is the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 power 2, which is 4. What about if we have more than one feature, For example Age and Income? For example, if we have income and age For each customer, we can still use the same formula, but this time in a 2-dimensional space. Also, we can use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose, but it is highly dependent on data type and also the domain that clustering is done For it. For example, you may use Euclidean distance, cosine similarity, average distance, and so on. Indeed, the similarity measure highly controls how the clusters are formed, so it is recommended to understand the domain knowledge of your dataset, and data type of features, and then choose the meaningful distance measurement. 

WORKING		:

1. Initialize k
	The key concept of the k-Means algorithm is that it randomly picks a center point For each cluster. It means, we must initialize k,which represents "number of clusters" There are two approaches to choose these centroids:
			a.  We can randomly choose 3 observations out of the dataset and use these observations as the initial means. 
			b.  We can create 3 random points as centroids of the clusters

2. Distance Calculation
	For this purpose, we have to calculate the distance of each data point (or in our case, each customer) from the centroid points.
	Therefore, you will form a matrix where each row represents the distance of a customer from each centroid. It is called the "distance-matrix."

3. Assign each point to the closest centroid. (Assign each customer to the closest center). - distance-matrix
	@The main objective of k_Means clustering Is to minimize the distance of data points From the centroid of its cluster And maximize the distance From other cluster centroids
	Find the closest centroid to each data point. We can use the distance-matrix to find the nearest centroid to data points. Finding the closest centroids For each data point, we assign each data point to that cluster. In other words, all the customers will fall to a cluster, based on their distance from centroids. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_distance_matrix.PNG"
	error is the total distance of each point from its centroid. It can be shown as within-cluster sum of squares error. 	
		SSE = Sum of squared distances between each point and its centroid.

		SSE =  Î£ ( xáµ¢ - Cáµ¢ )Â²
		 Intuitively, we try to reduce this error. It means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized.
 "How we can turn it into better clusters, with less error?"
4. Compute the new centroid For each cluster
	We move centroids. Each cluster center will be updated to be the" mean For data points" in its cluster. Indeed, each centroid moves according to their cluster members. In other words, the centroid of each of the 3 clusters becomes the new mean. 
	For example, if Point A coordination is 7.4 and 3.6, and Point B features are 7.8 and 3.8, the new centroid of this cluster with 2 points, would be the average of them, which is 7.6 and 3.7.

5. Repeat 2-4 untill there are no more changes
	Now we have new centroids. As you can guess, once again, we will have to calculate the distance of all points from the new centroids. The points are re-clustered and the centroids move again. This continues until the centroids no longer move. Please note that whenever a centroid moves, each pointâ€™s distance to the centroid needs to be measured again. Yes, k-Means is an iterative algorithm, and we have to repeat steps 2 to 4 until the algorithm converges. In each iteration, it will move the centroids, calculate the distances from new centroids, and assign the data points to the nearest centroid. It results in the clusters with minimum error, or the most dense clusters. However, as it is a heuristic algorithm, there is no guarantee that it will converge to the Global optimum, and the result may depend on the initial clusters. It means this algorithm is guaranteed to converge to a result, but the result may be a local optimum (i.e. not necessarily the best possible outcome). 



MORE ON K-MEANS 
Algorithm
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_Algo.PNG"


ACCURACY 	:
	 "How can we evaluate the 'goodness' of the clusters formed by k-Means?" In other words, 
	 "How do we calculate the accuracy of k-Means clustering?"

	 1. External approach 	
	 	Compare the clusters with groud truth, if available- (Mostly un-available since un-supervised data or un-labeled data)
	 
	 2. Internal approach
	 	This value is the "AVERAGE DISTANCE BETWEEN DATA POINTS WITHIN A CLUSTER". Also, average of the distances of data points from their cluster centroids can be used as a metric of error For the clustering algorithm.

CHOOSING K  :
	 The correct choice of k is often ambiguous, because itâ€™s very dependent on the shape and scale of the distribution of points in a data set. There are some approaches to address this problem, but one of the techniques that is commonly used, is to run the clustering across the different values of K, and looking at a metric of accuracy For clustering. This metric can be "â€œMEAN DISTANCE BETWEEN DATA POINTS AND THEIR CLUSTER CENTROID,â€ " which indicate how dense our clusters are, or to what extend we minimized the error of clustering. 
	 But the problem is that with increasing the number of clusters, the distance of centroids to data points will always reduce. This means, "increasing K will always decrease the â€œerror.â€" So, the value of the metric as a function of K is plotted and the "elbow point" is determined, where the rate of decrease sharply shifts. It is the right K For clustering. This method is called the "â€œELBOWâ€ METHOD."
	 "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Kmeans_Choosing_K_Elbow_method.PNG"


SUMMARY :
	k-Means is a partitioned-based clustering, which is:
		a. Relatively efficient on medium and large sized datasets;
		b. Produces sphere-like clusters, because the clusters are shaped around the centroids;
		c. Its drawback is that we should pre-specify the number of clusters-(k), and this is not an easy task.




LAB: K-MEANS
from sklearn.cluster import KMeans 

Some real-world applications of k-means:
	- Customer segmentation
	- Understand what the visitors of a website are trying to accomplish
	- Pattern recognition
	- Machine learning
	- Data compression

In this notebook we practice k-means clustering with 2 examples.
	- k-means on a random generated dataset	
	- Using k-means For customer segmentation

\k-Means on a randomly generated dataset
from sklearn.datasets.samples_generator import make_blobs 
We will be making random clusters of points by using the make_blobs class. The make_blobs class can take in many inputs, but we will be using these specific ones. 
"Input"
	n_samples: The total number of points equally divided among clusters.
		Value will be: 5000
	centers: The number of centers to generate, or the fixed center locations.
		Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]]
	cluster_std: The standard deviation of the clusters.
		Value will be: 0.9
"Output"
	X: Array of shape [n_samples, n_features]. (Feature Matrix)
		The generated samples.
	y: Array of shape [n_samples]. (Response Vector)
		The integer labels For cluster membership of each sample.


X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)
X, y
(array([[-0.91724214, -0.65544337],
        [-1.18588643, -0.82839009],
        [-0.14874474,  0.76529566],
        ...,
        [ 1.32068954,  0.97290081],
        [ 0.84875774, -3.40991974],
        [ 4.73720907,  3.05726509]]), array([1, 1, 3, ..., 3, 2, 0]))


plt.scatter(X[:, 0], X[:, 1], marker='.')

\Setting up K-Means

init: Initialization method of the centroids.
	Value will be: "k-means++"
	k-means++: Selects initial cluster centers For k-mean clustering in a smart way to speed up convergence.
n_clusters: The number of clusters to form as well as the number of centroids to generate.
	Value will be: 4 (since we have 4 centers)
n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.
	Value will be: 12

k_means = KMeans(init = "k-means++", n_clusters = 4, n_init = 12)
k_means.fit(X)
>>>
KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
    n_clusters=4, n_init=12, n_jobs=None, precompute_distances='auto',
    random_state=None, tol=0.0001, verbose=0)

k_means_labels = k_means.labels_
>>>
array([0, 3, 3, ..., 1, 0, 0], dtype=int32)

k_means_cluster_centers = k_means.cluster_centers_
>>>
array([[-2.03743147, -0.99782524],
       [ 3.97334234,  3.98758687],
       [ 0.96900523,  0.98370298],
       [ 1.99741008, -3.01666822]])


\Creating the Visual Plot - k-means
# Initialize the plot with the specified dimensions.
fig = plt.figure(figsize=(6, 4))

# Colors uses a color map, which will produce an array of colors based on
# the number of labels there are. We use set(k_means_labels) to get the
# unique labels.
colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))

# Create a plot
ax = fig.add_subplot(1, 1, 1)

# For loop that plots the data points and centroids.
# k will range from 0-3, which will match the possible clusters that each
# data point is in.
for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):

    # Create a list of all data points, where the data poitns that are 
    # in the cluster (ex. cluster 0) are labeled as true, else they are
    # labeled as false.
    my_members = (k_means_labels == k)
    
    # Define the centroid, or cluster center.
    cluster_center = k_means_cluster_centers[k]
    
    # Plots the datapoints with color col.
    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')
    
    # Plots the centroids with specified color, but with a darker outline
    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)

# Title of the plot
ax.set_title('KMeans')

# Remove x-axis ticks
ax.set_xticks(())

# Remove y-axis ticks
ax.set_yticks(())

# Show the plot
plt.show()



\Customer Segmentation with K-Means

import pandas as pd
cust_df = pd.read_csv("Cust_Segmentation.csv")
cust_df.head()
>>>
	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	Address	DebtIncomeRatio
0	1		 	41	2	6				19	   	0.124		1.073		0.0			NBA001	6.3
1	2		 	47	1	26				100	   	4.582		8.218		0.0			NBA021	12.8
2	3		 	33	2	10				57	   	6.111		5.802		1.0			NBA013	20.9
3	4		 	29	2	4				19	   	0.681		0.516		0.0			NBA009	6.3
4	5		 	47	1	31				253	   	9.308		8.908		0.0			NBA008	7.2

@PRE_PROCESSING
#As you can see, Address in this dataset is a categorical variable. k-means algorithm isn`t directly applicable to categorical variables because Euclidean distance function isn`t really meaningful For discrete variables. So, lets drop this feature and run clustering.
df = cust_df.drop('Address', axis=1)
df.head()
	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	DebtIncomeRatio
0	1		 	41	2	6				19	   	0.124		1.073		0.0			6.3
1	2		 	47	1	26				100	   	4.582		8.218		0.0			12.8
2	3		 	33	2	10				57	   	6.111		5.802		1.0			20.9
3	4		 	29	2	4				19	   	0.681		0.516		0.0			6.3
4	5		 	47	1	31				253	   	9.308		8.908		0.0			7.2



@NORMALIZING OVER THE STANDARD DEVIATION
# WHY ? Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use standardScaler() to normalize our dataset.
from sklearn.preprocessing import StandardScaler
X = df.values[:,1:]
X = np.nan_to_num(X)
Clus_dataSet = StandardScaler().fit_transform(X)
>>>
array([[ 0.74291541,  0.31212243, -0.37878978, ..., -0.59048916,
        -0.52379654, -0.57652509],
       [ 1.48949049, -0.76634938,  2.5737211 , ...,  1.51296181,
        -0.52379654,  0.39138677],
       [-0.25251804,  0.31212243,  0.2117124 , ...,  0.80170393,
         1.90913822,  1.59755385],
       ...,
       [-1.24795149,  2.46906604, -1.26454304, ...,  0.03863257,
         1.90913822,  3.45892281],
       [-0.37694723, -0.76634938,  0.50696349, ..., -0.70147601,
        -0.52379654, -1.08281745],
       [ 2.1116364 , -0.76634938,  1.09746566, ...,  0.16463355,
        -0.52379654, -0.2340332 ]])


@MODELING
#In our example (if we didn't have access to the k-means algorithm), it would be the same as guessing that each customer group would have certain age, income, education, etc, with multiple tests and experiments. However, using the K-means clustering we can do all this process much easier.
clusterNum = 3
k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means.fit(X)
labels = k_means.labels_
print(labels)
>>>
["0 2 0 0 1" 2 0 2 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 2 2 2 0 0 2 0 2 0 0 0 0 0 0
 0 0 2 0 2 0 1 0 2 0 0 0 2 2 0 0 2 2 0 0 0 2 0 2 0 2 2 0 0 2 0 0 0 2 2 2 0
 0 0 0 0 2 0 2 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 2 0
 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 2 0 2 0
 0 0 0 0 0 0 2 0 2 2 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 2 0
 0 0 0 0 2 0 0 2 0 2 0 0 2 1 0 2 0 0 0 0 0 0 1 2 0 0 0 0 2 0 0 2 2 0 2 0 2
 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 2 0 0 0 0
 0 0 2 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 2 0 2 0 2 2 0 0 0 0 0 0
 0 0 0 2 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 2 0 2 2 0
 0 0 0 0 2 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 0 2 0 0 0 1 0 0 0 2 0 2 2 2 0
 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0
 0 2 0 0 2 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 1
 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 2 0 1 0 0 0 0 2 0 2 2 2 0 0 2 2 0 0 0 0 0 0
 0 2 0 0 0 0 2 0 0 0 2 0 2 0 0 0 2 0 0 0 0 2 2 0 0 0 0 2 0 0 0 0 2 0 0 0 0
 0 2 2 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 1 0 1 0
 0 1 0 0 0 0 0 0 0 0 0 2 0 2 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 2
 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2
 2 0 0 2 0 2 0 0 2 0 2 0 0 1 0 2 0 2 0 0 0 0 0 2 2 0 0 0 0 2 0 0 0 2 2 0 0
 2 0 0 0 2 0 1 0 0 2 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0
 0 0 2 0 0 2 0 2 0 2 2 0 0 0 2 0 2 0 0 0 0 0 2 0 0 0 0 2 2 0 0 2 2 0 0 0 0
 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 0 2 0 2 2 0 0 2 0 0 0 0 0 2 2
 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2]

@INSIGHTS

 df["Clus_km"] = labels
 	Customer Id	Age	Edu	Years Employed	Income	Card Debt	Other Debt	Defaulted	DebtIncomeRatio \Clus_km
0	1		 	41	2	6				19	   	0.124		1.073		0.0			6.3				0
1	2		 	47	1	26				100	   	4.582		8.218		0.0			12.8			2
2	3		 	33	2	10				57	   	6.111		5.802		1.0			20.9			0
3	4		 	29	2	4				19	   	0.681		0.516		0.0			6.3				0
4	5		 	47	1	31				253	   	9.308		8.908		0.0			7.2				1


# check the centroid values by averaging the features in each cluster.
df.groupby('Clus_km').mean()
>>>
		Customer Id	Age			Edu			Years Employed	Income		Card Debt	Other Debt	Defaulted	DebtIncomeRatio
Clus_km									
0		432.468413	32.964561	1.614792	6.374422		31.164869	1.032541	2.104133	0.285185	10.094761
1		410.166667	45.388889	2.666667	19.555556		227.166667	5.678444	10.907167	0.285714	7.322222
2		402.295082	41.333333	1.956284	15.256831		83.928962	3.103639	5.765279	0.171233	10.724590




X[:, 0] = Age
X[:, 1] = Edu
X[:, 3] = Income
# IN 2d
# distribution of customers based on their age and income:
area = np.pi * ( X[:, 1])**2  # Edu is the size of scatter circle
plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(np.float), alpha=0.5)
plt.xlabel('Age', fontsize=18)
plt.ylabel('Income', fontsize=16)

plt.show()

# IN 3d
from mpl_toolkits.mplot3d import Axes3D 
fig = plt.figure(1, figsize=(8, 6))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()
# plt.ylabel('Age', fontsize=18)
# plt.xlabel('Income', fontsize=16)
# plt.zlabel('Education', fontsize=16)
ax.set_xlabel('Education')
ax.set_ylabel('Age')
ax.set_zlabel('Income')

ax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(np.float))

k-means will partition your customers into mutually exclusive groups, For example, into 3 clusters. The customers in each cluster are similar to each other demographically. Now we can create a profile For each group, considering the common characteristics of each cluster. For example, the 3 clusters can be:

AFFLUENT, EDUCATED AND OLD AGED
MIDDLE AGED AND MIDDLE INCOME
YOUNG AND LOW INCOME

**************************************************************************************
HIERARCHICAL CLUSTERING
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_dendogram_breed_full.jpg"
	This diagram, shows hierarchical clustering of these animals based on the similarity in their genetic data.


Hierarchical clustering algorithms build a hierarchy of clusters where each node is a cluster consisting of the clusters of its daughter nodes. 

Strategies For hierarchical clustering generally fall into two types: 
	1. Divisive
		Divisive is top-down, so you start with all observations in a large cluster and break it down into smaller pieces. Think about divisive as "dividing" the cluster.
	2. Agglomerative.
		Agglomerative is the opposite of divisive, so it is bottom-up, where each observation starts in its own cluster and pairs of clusters are merged together as they move up the hierarchy. Agglomeration means to amass or collect things, which is exactly what this does with the cluster.


Agglomerative clustering.
This method builds the hierarchy from the individual elements by progressively merging clusters. 

EXAMPLE : letâ€™s say we want to cluster 6 cities in Canada based on their distances from one another.
They are: Toronto, Ottawa, Vancouver, Montreal, Winnipeg, and Edmonton. We construct a distance matrix at this stage, where the numbers in the row i column j is the distance between the i and j cities. In fact, this table shows the distances between each pair of cities. The algorithm is started by assigning each city to its own cluster. So, if we have 6 cities, we have 6 clusters, each containing just one city. Letâ€™s note each city by showing the first two characters of its name. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram.PNG"
	The first step is to determine which cities to merge into a cluster. 
		Usually, we want to take the two closest clusters according to the chosen distance. Looking at the distance matrix, Montreal and Ottawa are the closest clusters. So, we make a cluster out of them. Please notice that we just use a simple 1-dimentional distance feature here, but our object can be multi-dimensional, and distance measurement can be either Euclidean, Pearson, average distance, or many others, depending on data type and domain knowledge. Anyhow, we have to merge these two closest cities in the distance matrix as well. So, rows and columns are merged as the cluster is constructed. As you can see in the distance matrix, rows and columns related to Montreal and Ottawa cities are merged as the cluster is constructed. Then, the distances from all cities to this new merged cluster get updated. But how? For example, how do we calculate the distance from Winnipeg to the Ottawa-Montreal cluster? Well, there are different approaches, but letâ€™s assume, For example, we just select the distance from the centre of the Ottawa-Montreal cluster to Winnipeg. Updating the distance matrix, we now have one less cluster. Next, we look For the closest clusters once again. In this case, Ottawa-Montreal and Toronto are the closest ones, which creates another cluster. In the next step, the closest distance is between the Vancouver cluster and the Edmonton cluster. Forming a new cluster, their data in the matrix table gets updated. Essentially, the rows and columns are merged as the clusters are merged and the distance updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. In the same way, agglomerative algorithm proceeds by merging clusters. And we repeat it until all clusters are merged and the tree becomes completed. It means, until all cities are clustered into a single cluster of size 6. 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram_1.PNG"
	- Hierarchical clustering is typically visualized as a dendrogram as shown on this slide. 
	- Each merge is represented by a horizontal line. 
	- The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. 
	- By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. 
	- Essentially, Hierarchical clustering does not require a pre-specified number of clusters.

	- However, in some applications we want a partition of disjoint clusters just as in flat clustering. 
		In those cases, the hierarchy needs to be cut at some point. 
		For example here, cutting in a specific level of similarity, we create 3 clusters of similar cities.
   "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_dendogram_2.PNG"




MORE ON HIERARCHICAL CLUSTERING
Agglomerative algorithm for Hierarchical Clustering. Remember that Agglomerative clustering is a bottom-up approach.
Letâ€™s say our dataset has n data points. 
	1. "Create n clusters, one for each data point". Then each point is assigned as a cluster.
	2. "Compute the distance/proximity matrix", which will be an n by n table.
	3. REPEAT : (until the specified cluster number is reached, or until there is only one cluster left.)
			i.  "MERGE the two nearest clusters". (Distances are computed already in the proximity matrix.)
			ii. "UPDATE the proximity matrix" with the new values.
	4. UNTIL only single cluster remains 

So, in the proximity matrix, we have to measure the distances between clusters, and also merge the clusters that are â€œnearest.â€ So, the key operation is the computation of the proximity between the clusters with one point, and also clusters with multiple data points.

A. "How do we measure the distances between these clusters and How do we define the â€˜nearestâ€™ among clusters?"
B. "Which points do we use?"

First, letâ€™s see how to calculate the distance between 2 clusters with 1 point each. Letâ€™s assume that we have a dataset of patients, and we want to cluster them using hierarchy clustering. So, our data points are patients, with a feature set of 3 dimensions. For example, Age, Body Mass Index (or BMI), and Blood Pressure. We can use different distance measurements to calculate the proximity matrix. For instance, Euclidean distance. So, if we have a dataset of n patients, we can build an n by n dissimilarly-distance matrix. It will give us the distance of clusters with 1 data point. 
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_SimilarityMatrix.PNG"

However, as mentioned, we merge clusters in Agglomerative clustering. Now, the question is, "How can we calculate the distance between clusters when there are multiple patients in each cluster?"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_AgglomerativeC_SimilarityMatrix_betweenClusters.PNG"	


	1. Single-linkage clustering
		Minimum distance between clusters

	2. Complete-linkage clustering
		Maximum distance between clusters

	3. Average-linkage clustering
		Average distance between clusters

	4. Centroid-linkage clustering
		Distance between cluster centroids

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_Clustering_linkage.PNG"


Advantages - Disadvantages 
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Adv_DisAdv.PNG"

Heirarchical Clustering vs Kmeans  
	"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Vs_Kmeans.PNG"








LAB: HIERARCHICAL CLUSTERING
from scipy import ndimage 
from scipy.cluster import hierarchy 
from scipy.spatial import distance_matrix 
from sklearn import manifold, datasets 
from sklearn.cluster import AgglomerativeClustering


X1, y1 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)
>>>
(array([[-1.45954024, -1.53847475],
        [ 0.89359214,  1.43030718],
        [10.91585682,  3.09954789],
        [10.82577693,  3.17694028],
        [11.00075809,  4.78531239],
        [ 3.33876114,  3.21752034],
        [ 5.16655142,  3.8716683 ],
        [ 0.58304393,  1.57176676],
        [-3.27035829, -0.63428103],
        [-1.81705689, -0.03322545],
        [-2.80800077, -1.74855626],
        [ 9.46146088,  5.31818268],
        [-1.84978113, -0.57163706],
        [ 3.29674917,  3.15260712],
        [ 0.22848229,  0.70372806],
        [ 9.21399684,  3.00513502],
        [ 2.02027541,  1.82341234],
        [10.10415481,  3.77993715],
        [-2.09909309, -1.04208023],
        [ 4.24253686,  2.76154769],
        [-3.61785098,  0.66950408],
        [ 1.01827269,  0.49572229],
        [-3.36405167, -0.50019257],
        [10.17527213,  3.52550805],
        [-1.2830054 ,  0.8747671 ],
        [-2.5453101 , -2.65045066],
        [10.23193458,  4.15809891],
        [ 9.89213364,  4.51506412],
        [ 4.14512341,  2.85604029],
        [-1.5230482 , -1.40333333],
        [ 9.98837086,  3.14411953],
        [ 3.23553758,  3.10821041],
        [ 5.0467893 ,  5.54561749],
        [ 5.58643251,  4.29407544],
        [ 9.98243961,  3.03607186],
        [10.89566718,  3.2124786 ],
        [ 1.95914382,  0.9279169 ],
        [ 1.21129456,  1.94866486],
        [ 1.97658783, -0.38749302],
        [-1.17924668, -1.72614974],
        [ 0.74870593,  0.67163472],
        [-3.81481406, -0.82770804],
        [ 3.36689786,  4.27319337],
        [ 4.79442499,  4.47331649],
        [ 6.20735126,  4.40336371],
        [-0.16757584,  1.52674504],
        [-2.68649102, -1.60938839],
        [ 4.1990383 ,  2.88396003],
        [ 3.24503419,  4.26231564],
        [ 0.5625057 ,  1.34677072]]),
 array([1, 2, 3, 3, 3, 0, 0, 2, 1, 1, 1, 3, 1, 0, 2, 3, 2, 3, 1, 0, 1, 2,
        1, 3, 2, 1, 3, 3, 0, 1, 3, 0, 0, 0, 3, 3, 2, 2, 2, 1, 2, 1, 0, 0,
        0, 2, 1, 0, 0, 2]))

\Agglomerative Clustering
The Agglomerative Clustering class will require two inputs:
"n_clusters": The number of clusters to form as well as the number of centroids to generate. 
	Value will be: 4
"linkage": Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluste that minimize this criterion.
	Value will be: 'complete'

from sklearn.cluster import AgglomerativeClustering	
agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')

agglom.fit(X1,y1)
>>>
	AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
            connectivity=None, linkage='average', memory=None,
            n_clusters=4, pooling_func='deprecated')

# Create a figure of size 6 inches by 4 inches.
plt.figure(figsize=(6,4))

# These two lines of code are used to scale the data points down,
# Or else the data points will be scattered very far apart.

# Create a minimum and maximum range of X1.
x_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)

# Get the average distance for X1.
X1 = (X1 - x_min) / (x_max - x_min)

# This loop displays all of the datapoints.
for i in range(X1.shape[0]):
    # Replace the data points with their respective cluster value 
    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)
    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),
             color=plt.cm.nipy_spectral(agglom.labels_[i] / 10.),
             fontdict={'weight': 'bold', 'size': 9})
    
# Remove the x ticks, y ticks, x and y axis
plt.xticks([])
plt.yticks([])
#plt.axis('off')



# Display the plot of the original data before clustering
plt.scatter(X1[:, 0], X1[:, 1], marker='.')
# Display the plot
plt.show()


\Dendrogram Associated for the Agglomerative Hierarchical Clustering
Remember that a distance matrix contains the distance from each point to every other point of a dataset . 
Use the function distance_matrix, which requires two inputs. Use the Feature Matrix, X2 as both inputs and save the distance matrix to a variable called dist_matrix 

from scipy.spatial import distance_matrix 
dist_matrix = distance_matrix(X1,X1) 
print(dist_matrix)
[[0.         0.39551226 1.00893258 ... 0.66107534 0.77572375 0.37755901]
 [0.39551226 0.         0.70646175 ... 0.28501381 0.38024066 0.02456174]
 [1.00893258 0.70646175 0.         ... 0.4541245  0.53683896 0.73080599]
 ...
 [0.66107534 0.28501381 0.4541245  ... 0.         0.18007891 0.30890636]
 [0.77572375 0.38024066 0.53683896 ... 0.18007891 0.         0.39915334]
 [0.37755901 0.02456174 0.73080599 ... 0.30890636 0.39915334 0.        ]]
	(50, 50)

from scipy.cluster import hierarchy
Z = hierarchy.linkage(dist_matrix, 'complete')
>>>
array([[2.80000000e+01, 4.70000000e+01, 3.14668251e-02, 2.00000000e+00],
       [3.00000000e+00, 3.50000000e+01, 3.67496046e-02, 2.00000000e+00],
       [4.20000000e+01, 4.80000000e+01, 4.25797489e-02, 2.00000000e+00],
       [1.30000000e+01, 3.10000000e+01, 4.27725050e-02, 2.00000000e+00],
       [2.00000000e+00, 5.10000000e+01, 4.76077809e-02, 3.00000000e+00],
       [3.00000000e+01, 3.40000000e+01, 5.12089976e-02, 2.00000000e+00],
       [1.90000000e+01, 5.00000000e+01, 6.34388670e-02, 3.00000000e+00],
       [8.00000000e+00, 2.20000000e+01, 6.38503010e-02, 2.00000000e+00],
       [0.00000000e+00, 2.90000000e+01, 8.34788906e-02, 2.00000000e+00],
       [1.00000000e+00, 7.00000000e+00, 8.44319620e-02, 2.00000000e+00],
       [5.00000000e+00, 5.30000000e+01, 9.03341009e-02, 3.00000000e+00],
       [2.10000000e+01, 4.00000000e+01, 9.40410671e-02, 2.00000000e+00],
       [1.70000000e+01, 2.30000000e+01, 1.21572734e-01, 2.00000000e+00],
       [1.00000000e+01, 4.60000000e+01, 1.22146049e-01, 2.00000000e+00],
       [4.90000000e+01, 5.90000000e+01, 1.51506895e-01, 3.00000000e+00],
       [2.60000000e+01, 2.70000000e+01, 1.71643832e-01, 2.00000000e+00],
       [3.90000000e+01, 5.80000000e+01, 1.85582180e-01, 3.00000000e+00],
       [1.40000000e+01, 6.10000000e+01, 2.05873644e-01, 3.00000000e+00],
       [1.60000000e+01, 3.70000000e+01, 2.32985204e-01, 2.00000000e+00],
       [3.30000000e+01, 4.30000000e+01, 2.44103023e-01, 2.00000000e+00],
       [5.60000000e+01, 6.00000000e+01, 2.91367379e-01, 6.00000000e+00],
       [4.10000000e+01, 5.70000000e+01, 3.11408745e-01, 3.00000000e+00],
       [4.50000000e+01, 6.40000000e+01, 3.32461531e-01, 4.00000000e+00],
       [9.00000000e+00, 1.20000000e+01, 3.43204499e-01, 2.00000000e+00],
       [6.00000000e+00, 6.90000000e+01, 3.48464439e-01, 3.00000000e+00],
       [1.50000000e+01, 5.50000000e+01, 3.48642921e-01, 3.00000000e+00],
       [5.40000000e+01, 6.20000000e+01, 3.52125944e-01, 5.00000000e+00],
       [1.80000000e+01, 6.60000000e+01, 3.56302912e-01, 4.00000000e+00],
       [4.00000000e+00, 1.10000000e+01, 4.28945893e-01, 2.00000000e+00],
       [3.60000000e+01, 7.20000000e+01, 5.11093439e-01, 5.00000000e+00],
       [6.50000000e+01, 7.80000000e+01, 5.57384557e-01, 4.00000000e+00],
       [7.10000000e+01, 7.70000000e+01, 5.71067795e-01, 7.00000000e+00],
       [3.80000000e+01, 6.70000000e+01, 5.87373820e-01, 4.00000000e+00],
       [4.40000000e+01, 7.40000000e+01, 5.88909578e-01, 4.00000000e+00],
       [2.00000000e+01, 7.30000000e+01, 5.95242496e-01, 3.00000000e+00],
       [6.30000000e+01, 8.10000000e+01, 6.33397563e-01, 9.00000000e+00],
       [2.40000000e+01, 8.20000000e+01, 7.08469459e-01, 5.00000000e+00],
       [7.50000000e+01, 7.60000000e+01, 7.10534758e-01, 8.00000000e+00],
       [5.20000000e+01, 7.00000000e+01, 7.62151961e-01, 8.00000000e+00],
       [6.80000000e+01, 7.90000000e+01, 8.51494356e-01, 7.00000000e+00],
       [3.20000000e+01, 8.30000000e+01, 1.01602852e+00, 5.00000000e+00],
       [2.50000000e+01, 8.50000000e+01, 1.15906592e+00, 1.00000000e+01],
       [8.00000000e+01, 8.70000000e+01, 1.27578019e+00, 1.20000000e+01],
       [8.60000000e+01, 8.90000000e+01, 1.48733527e+00, 1.20000000e+01],
       [8.40000000e+01, 9.10000000e+01, 1.73813657e+00, 1.30000000e+01],
       [8.80000000e+01, 9.00000000e+01, 1.78183746e+00, 1.30000000e+01],
       [9.20000000e+01, 9.50000000e+01, 3.18041372e+00, 2.50000000e+01],
       [9.30000000e+01, 9.40000000e+01, 3.45885459e+00, 2.50000000e+01],
       [9.60000000e+01, 9.70000000e+01, 5.40273377e+00, 5.00000000e+01]])
(49, 4)


A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering.

Next, we will save the dendrogram to a variable called dendro. In doing this, the dendrogram will also be displayed. Using the dendrogram class from hierarchy, pass in the parameter:  Z

dendro = hierarchy.dendrogram(Z)
"https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Code_dendogram.png"

----------------------------------------------------------------------------------
\Clustering on Vehicle dataset
------------------------------------------------------------------------------------------------
pdf = pd.read_csv('cars_clus.csv')
>>>
Shape of dataset:  (159, 16)
manufact	model	sales	resale	type	price	engine_s	horsepow	wheelbas	width	length	curb_wgt	fuel_cap	mpg		lnsales	partition
0	Acura	Integra	16.919	16.360	0.000	21.500	1.800		140.000	     101.200	67.300	172.400	2.639	     13.200	    28.000	2.828	0.0
1	Acura	TL		39.384	19.875	0.000	28.400	3.200		225.000	     108.100	70.300	192.900	3.517	     17.200	    25.000	3.673	0.0
2	Acura	CL		14.114	18.225	0.000	 ð‘›ð‘¢ð‘™ð‘™ 	3.200		225.000	     106.900	70.600	192.000	3.470	     17.200	    26.000	2.647	0.0
3	Acura	RL		8.588	29.725	0.000	42.000	3.500		210.000	     114.600	71.400	196.600	3.850	     18.000	    22.000	2.150	0.0
4	Audi	A4		20.397	22.255	0.000	23.990	1.800		150.000	     102.600	68.200	178.000	2.998	     16.400	    27.000	3.015	0.0

The featuresets include price in thousands (price), engine size (engine_s), horsepower (horsepow), wheelbase (wheelbas), width (width), length (length), curb weight (curb_wgt), fuel capacity (fuel_cap) and fuel efficiency (mpg).


\Data Cleaning
print ("Shape of dataset before cleaning: ", pdf.size)
pdf[[ 'sales', 'resale', 'type', 'price', 'engine_s',
       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',
       'mpg', 'lnsales']] = pdf[['sales', 'resale', 'type', 'price', 'engine_s',
       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',
       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')
pdf = pdf.dropna()
pdf = pdf.reset_index(drop=True)
print ("Shape of dataset after cleaning: ", pdf.size)
pdf.head(5)
Shape of dataset before cleaning:  2544
Shape of dataset after cleaning:  1872

\Feature selection
featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]


\Normalization
"MinMaxScaler" transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one.
from sklearn.preprocessing import MinMaxScaler
x = featureset.values #returns a numpy array
min_max_scaler = MinMaxScaler()
feature_mtx = min_max_scaler.fit_transform(x)
feature_mtx [0:5]
>>>
array([[0.11428571, 0.21518987, 0.18655098, 0.28143713, 0.30625832,
        0.2310559 , 0.13364055, 0.43333333],
       [0.31428571, 0.43037975, 0.3362256 , 0.46107784, 0.5792277 ,
        0.50372671, 0.31797235, 0.33333333],
       [0.35714286, 0.39240506, 0.47722343, 0.52694611, 0.62849534,
        0.60714286, 0.35483871, 0.23333333],
       [0.11428571, 0.24050633, 0.21691974, 0.33532934, 0.38082557,
        0.34254658, 0.28110599, 0.4       ],
       [0.25714286, 0.36708861, 0.34924078, 0.80838323, 0.56724368,
        0.5173913 , 0.37788018, 0.23333333]])

------------------------------------------------------------------------------------------------
\Clustering using Scipy
------------------------------------------------------------------------------------------------
import scipy
leng = feature_mtx.shape[0]
D = scipy.zeros([leng,leng])
for i in range(leng):
    for j in range(leng):
        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])


linkage -->  single - complete - average - weighted - centroid

import pylab
import scipy.cluster.hierarchy
Z = hierarchy.linkage(D, 'complete')

from scipy.cluster.hierarchy import fcluster
max_d = 3
clusters = fcluster(Z, max_d, criterion='distance')
>>>
array([ 1,  5,  5,  6,  5,  4,  6,  5,  5,  5,  5,  5,  4,  4,  5,  1,  6,
        5,  5,  5,  4,  2, 11,  6,  6,  5,  6,  5,  1,  6,  6, 10,  9,  8,
        9,  3,  5,  1,  7,  6,  5,  3,  5,  3,  8,  7,  9,  2,  6,  6,  5,
        4,  2,  1,  6,  5,  2,  7,  5,  5,  5,  4,  4,  3,  2,  6,  6,  5,
        7,  4,  7,  6,  6,  5,  3,  5,  5,  6,  5,  4,  4,  1,  6,  5,  5,
        5,  6,  4,  5,  4,  1,  6,  5,  6,  6,  5,  5,  5,  7,  7,  7,  2,
        2,  1,  2,  6,  5,  1,  1,  1,  7,  8,  1,  1,  6,  1,  1],
      dtype=int32)

from scipy.cluster.hierarchy import fcluster
k = 5
clusters = fcluster(Z, k, criterion='maxclust')
>>>
array([1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 1, 3, 3, 3, 3, 2, 1,
       5, 3, 3, 3, 3, 3, 1, 3, 3, 4, 4, 4, 4, 2, 3, 1, 3, 3, 3, 2, 3, 2,
       4, 3, 4, 1, 3, 3, 3, 2, 1, 1, 3, 3, 1, 3, 3, 3, 3, 2, 2, 2, 1, 3,
       3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 2,
       3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1,
       3, 4, 1, 1, 3, 1, 1], dtype=int32)



fig = pylab.figure(figsize=(18,50))
def llf(id):
    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )
    
dendro = hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')
"https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Code_dendogram_1.png"

------------------------------------------------------------------------------------------------
\Clustering using scikit-learn
------------------------------------------------------------------------------------------------
from scipy.spatial import distance_matrix
dist_matrix = distance_matrix(feature_mtx,feature_mtx) 
print(dist_matrix)
>>>
[[0.         0.57777143 0.75455727 ... 0.28530295 0.24917241 0.18879995]
 [0.57777143 0.         0.22798938 ... 0.36087756 0.66346677 0.62201282]
 [0.75455727 0.22798938 0.         ... 0.51727787 0.81786095 0.77930119]
 ...
 [0.28530295 0.36087756 0.51727787 ... 0.         0.41797928 0.35720492]
 [0.24917241 0.66346677 0.81786095 ... 0.41797928 0.         0.15212198]
 [0.18879995 0.62201282 0.77930119 ... 0.35720492 0.15212198 0.        ]]


Now, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset. The AgglomerativeClustering performs a hierarchical clustering using a bottom up approach. The linkage criteria determines the metric used for the merge strategy:

- "Ward" minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.
- Maximum or "complete" linkage minimizes the maximum distance between observations of pairs of clusters.
- "Average" linkage minimizes the average of the distances between all observations of pairs of clusters.

1. Distance Metrics - (affinity)  : Distance metrics can be set as either Euclidean, Manhattan, or Cosine
2. Linkage 			- (linkage)	  : The linkage parameters are Ward, Complete, and Average. 


agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')
agglom.fit(feature_mtx)
agglom.labels_
>>>
array([1, 2, 2, 1, 2, 3, 1, 2, 2, 2, 2, 2, 3, 3, 2, 1, 1, 2, 2, 2, 5, 1,
       4, 1, 1, 2, 1, 2, 1, 1, 1, 5, 0, 0, 0, 3, 2, 1, 2, 1, 2, 3, 2, 3,
       0, 3, 0, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1,
       1, 2, 1, 2, 2, 1, 1, 2, 3, 2, 3, 1, 2, 3, 5, 1, 1, 2, 3, 2, 1, 3,
       2, 3, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,
       2, 0, 1, 1, 1, 1, 1])
pdf['cluster_'] = agglom.labels_
>>>
manufact	model	sales	resale	type	price	engine_s	horsepow	wheelbas	width	length	curb_wgt	fuel_cap	mpg		lnsales	partition cluster_
0	Acura	Integra	16.919	16.360	0.000	21.500	1.800		140.000	     101.200	67.300	172.400	2.639	     13.200	    28.000	2.828	0.0			1
1	Acura	TL		39.384	19.875	0.000	28.400	3.200		225.000	     108.100	70.300	192.900	3.517	     17.200	    25.000	3.673	0.0			2
2	Acura	CL		14.114	18.225	0.000	 ð‘›ð‘¢ð‘™ð‘™ 	3.200		225.000	     106.900	70.600	192.000	3.470	     17.200	    26.000	2.647	0.0			2
3	Acura	RL		8.588	29.725	0.000	42.000	3.500		210.000	     114.600	71.400	196.600	3.850	     18.000	    22.000	2.150	0.0			1
4	Audi	A4		20.397	22.255	0.000	23.990	1.800		150.000	     102.600	68.200	178.000	2.998	     16.400	    27.000	3.015	0.0			2


import matplotlib.cm as cm
n_clusters = max(agglom.labels_)+1
colors = cm.rainbow(np.linspace(0, 1, n_clusters))
cluster_labels = list(range(0, n_clusters))

# Create a figure of size 6 inches by 4 inches.
plt.figure(figsize=(16,14))

for color, label in zip(colors, cluster_labels):
    subset = pdf[pdf.cluster_ == label]
    for i in subset.index:
            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) 
    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)
#    plt.scatter(subset.horsepow, subset.mpg)
plt.legend()
plt.title('Clusters')
plt.xlabel('horsepow')
plt.ylabel('mpg')

Text(0, 0.5, 'mpg')
"https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Scikit_scatter.png"
As you can see, we are seeing the distribution of each cluster using the scatter plot, but it is not very clear where is the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, "truck" (value of 0 in the type column) and "car" (value of 1 in the type column). So, we use them to distinguish the classes, and summarize the cluster. First we count the number of cases in each group:

pdf.groupby(['cluster_','type'])['cluster_'].count()
agg_cars = pdf.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()
agg_cars
>>>
						horsepow	engine_s	mpg			price
cluster_	type				
		0	1.0			211.666667	4.483333	16.166667	29.024667
		1	0.0			146.531915	2.246809	27.021277	20.306128
			1.0			145.000000	2.580000	22.200000	17.009200
		2	0.0			203.111111	3.303704	24.214815	27.750593
			1.0			182.090909	3.345455	20.181818	26.265364
		3	0.0			256.500000	4.410000	21.500000	42.870400
			1.0			160.571429	3.071429	21.428571	21.527714
		4	0.0			55.000000	1.000000	45.000000	9.235000
		5	0.0			365.666667	6.233333	19.333333	66.010000

It is obvious that we have 3 main clusters with the majority of vehicles in those.

Cars:

Cluster 1: with almost high mpg, and low in horsepower.
Cluster 2: with good mpg and horsepower, but higher price than average.
Cluster 3: with low mpg, high horsepower, highest price.
Trucks:

Cluster 1: with almost highest mpg among trucks, and lowest in horsepower and price.
Cluster 2: with almost low mpg and medium horsepower, but higher price than average.
Cluster 3: with good mpg and horsepower, low price.
Please notice that we did not use type , and price of cars in the clustering process, but Hierarchical clustering could forge the clusters and discriminate them with quite high accuracy.

plt.figure(figsize=(16,10))
for color, label in zip(colors, cluster_labels):
    subset = agg_cars.loc[(label,),]
    for i in subset.index:
        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')
    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))
plt.legend()
plt.title('Clusters')
plt.xlabel('horsepow')
plt.ylabel('mpg')
"https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/5-Clustering/Heirarchical_clustering_Scikit_scatter_1.png"



**************************************************************************************
DBSCAN CLUSTERING

LAB: DBSCAN CLUSTERING

**************************************************************************************
GRADED REVIEW QUESTIONS
