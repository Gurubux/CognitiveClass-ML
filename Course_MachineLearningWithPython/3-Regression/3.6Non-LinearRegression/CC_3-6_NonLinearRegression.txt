Let me answer two important questions here: 

First, “How can I know if a problem is linear or non-linear in an easy way?”
To answer this question, we have to do two things:
	The first is to visually figure out if the relation is linear or non-linear. It’s best to plot bivariate plots of output variables with each input variable.
	Also, you can calculate the correlation coefficient between independent and dependent variables, and if for all variables it is 0.7 or higher there is a linear tendency, and, thus, it’s not appropriate to fit a non-linear regression.

	The second thing we have to do is to use non-linear regression instead of linear regression when we cannot accurately model the relationship with linear parameters.

The second important questions is, “How should I model my data, if it displays non-linear on a scatter plot?”
	Well, to address this, you have to use either a polynomial regression, use a non-linear regression model, or "transform" your data, which is not in scope for this course.

--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
1. Non-linear functions can have elements like exponentials, logarithms, fractions, and others. For example: 
		y = log(x)
	Please consider that instead of x, we can use X, which can be polynomial representation of the x`s. In general form it would be written as
		y = log(X)
	X = np.arange(-5.0, 5.0, 0.1)

	Y = np.log(X)
	
	plt.plot(X,Y) 
	plt.ylabel('Dependent Variable')
	plt.xlabel('Indepdendent Variable')
	plt.show()


2. Or even, more complicated such as : 
		y = log(a x^3 + b x^2 + c x + d)

3. Cubic Function
		y = a x^3 + b x^2 + c x + d	
	x = np.arange(-5.0, 5.0, 0.1)

	##You can adjust the slope and intercept to verify the changes in the graph
	y = 1*(x**3) + 1*(x**2) + 1*x + 3
	y_noise = 20 * np.random.normal(size=x.size)
	ydata = y + y_noise
	plt.plot(x, ydata,  'bo')
	plt.plot(x,y, 'r') 
	plt.ylabel('Dependent Variable')
	plt.xlabel('Indepdendent Variable')
	plt.show()



4. Quadratic
		Y = X^2

	x = np.arange(-5.0, 5.0, 0.1)

	##You can adjust the slope and intercept to verify the changes in the graph

	y = np.power(x,2)
	y_noise = 2 * np.random.normal(size=x.size)
	ydata = y + y_noise
	plt.plot(x, ydata,  'bo')
	plt.plot(x,y, 'r') 
	plt.ylabel('Dependent Variable')
	plt.xlabel('Indepdendent Variable')
	plt.show()

5. Exponential
	An exponential function with base c is defined by 
		Y = a + b c^X
			where b ≠0, c > 0, c ≠1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.
	X = np.arange(-5.0, 5.0, 0.1)
	
	##You can adjust the slope and intercept to verify the changes in the graph
	
	Y= np.exp(X)
	
	plt.plot(X,Y) 
	plt.ylabel('Dependent Variable')
	plt.xlabel('Indepdendent Variable')
	plt.show()


6. Sigmoidal/Logistic
			       	b
	Y = 	a + -----------
				1 + c⁽ˣ⁻ᵈ⁾

7. The formula for the logistic function is the following:
			       	1
	Y = 	 	-----------
				      β₁(X⁻β₂)
				1 + e
	β₁ : Controls the curve`s steepness.
	β₂ : Slides the curve on the x-axis.
	
	def sigmoid(x, Beta_1, Beta_2):
		y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
		return y

	#logistic function
	Y_pred = sigmoid(x_data, beta_1 , beta_2)
	
	#plot initial prediction against datapoints
	plt.plot(x_data, Y_pred*15000000000000.)
	plt.plot(x_data, y_data, 'ro')


--------------------------------------------------------------------------------------
https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/3-Regression/3.6Non-LinearRegression/CC_3_6_NonLinearRegression.ipynb

How we find the best parameters for our fit line?
we can use curve_fit which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.

"popt" is our optimized parameter/s.


	from scipy.optimize import curve_fit
	popt, pcov = curve_fit(sigmoid, xdata, ydata)
	#print the final parameters
	print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
>>>beta_1 = 690.453017, beta_2 = 0.997207

#Now we plot our resulting regresssion model.
	x = np.linspace(1960, 2015, 55)
	x = x/max(x)
	plt.figure(figsize=(8,5))
	y = sigmoid(x, *popt)
	plt.plot(xdata, ydata, 'ro', label='data')
	plt.plot(x,y, linewidth=3.0, label='fit')
	plt.legend(loc='best')
	plt.ylabel('GDP')
	plt.xlabel('Year')
	plt.show()