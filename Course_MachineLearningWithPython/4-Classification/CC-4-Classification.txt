INTRO TO CLASSIFICATION

K-NEAREST NEIGHBORS
EVALUATION METRICS
LAB: KNN

INTRO TO DECISION TREES
BUILDING DECISION TREES
LAB: DECISION TREES

INTRO TO LOGISTIC REGRESSION
LOGISTIC VS LINEAR REGRESSION
LAB: LOGISTIC REGRESSION

SUPPORT VECTOR MACHINE
LAB: SUPPORT VECTOR MACHINES

GRADED REVIEW QUESTIONS

**************************************************************************************
INTRO TO CLASSIFICATION
In Machine Learning, classification is a supervised learning approach, which can be thought of as a means of categorizing or "classifying" some unknown items into a discrete set of "classes."
The goal of a loan default predictor is to use existing loan default data, which is information about the customers (such as age, income, education, etc.), to build a classifier, pass a new customer or potential future defaulter to the model, and then label it (i.e. the data points) as "Defaulter" or "Not Defaulter", or for example, 0 or 1. This is how a classifier predicts an unlabeled test case.
We can also build classifier models For both binary classification and multi-class classification.

Business Use-cases : 
	To predict the category to which a customer belongs;
	For Churn detection, where we predict whether a customer switches to another provider or brand; 
	Or to predict whether or not a customer responds to a particular advertising campaign.

Applications :
	email filtering, 
	speech recognition, 
	handwriting recognition, 
	bio-metric identification, 
	document classification, and much more.

Algorithms : 
	1. Decision Trees, ✔
	2. Naïve Bayes, 
	3. Linear Discriminant Analysis, 
	4. K-nearest neighbor,✔
	5. Logistic regression, ✔
	6. Neural Networks, 
	7. Support Vector Machines.✔

**************************************************************************************
K-NEAREST NEIGHBORS
The example focuses on using demographic data, such as region, age, and marital status, to predict usage patterns. The target field, called custcat, has four possible values that correspond to the four customer groups, as follows: Basic Service, E-Service, Plus Service, and Total Service. Our objective is to build a classifier, for example using the rows 0 to 7, to predict the class of row 8.

"How can we find the class of this customer?	"
	Can we find one of the closest cases and assign the same class label to our new customer?	Can we also say that the class of our new customer is most probably group 4 (i.e. total	service) because its nearest neighbor is also of "class 4"?	Yes, we can. In fact, it is the first-nearest neighbor.

"To what extent can we trust our judgment, which is based on the first nearest neighbor?"
	It might be a poor judgment, especially if the first nearest neighbor is a very specific case, or an "outlier", correct? Now, let’s look at our scatter plot again. Rather than choose the first nearest neighbor, what" if we chose the five nearest neighbors", and did a "majority vote" among them to define the class of our new customer? In this case, we’d see that three out of five nearest neighbors tell us to go For "class 3", which is ”Plus service.”
	In this case, the value of K in the k-nearest neighbors algorithm is 5.

K-NEAREST NEIGHBORS. 
	The k-nearest-neighbors algorithm is a classification algorithm that takes a bunch of labelled points and uses them to learn how to label other points. 
	This algorithm classifies cases based on their similarity to other cases. In k-nearest neighbors, data points that are near each other are said to be "neighbors". 
	K-nearest neighbors is based on this paradigm: "“Similar cases with the same class labels are near each other.”" Thus, the distance between two cases is a measure of their dissimilarity. 
	There are different ways to calculate the similarity, or conversely, the distance or dissimilarity of two data points. For example, this can be done using  "Euclidian distance".

K-NN Algorithm working :
	1. Pick a value For K.
	2. Calculate the distance of unknown case from all cases (holdout from each of the cases in the dataset).
	3. Search for the K observations in the training data that are ‘nearest’ to the measurements of the unknown data point.
	4. Predict the response of the unknown data point using the "most popular response" value from the K nearest neighbors.

There are two parts in this algorithm that might be a bit confusing.
	First, "how to select the correct K";
	Second, "how to compute the similarity between cases", for example, among customers?

	"how to compute the similarity between cases"
		Assume that we have two customers, customer 1 and customer 2. And, For a moment, assume that these 2 customers have only one feature, Age. We can easily use a specific type of Minkowski distance to calculate the distance of these 2 customers. It is indeed, the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 to power of 2, which is 4. What about if we have more than one feature, for example Age and Income? If we have income and age For each customer, we can still use the same formula, but this time, we’re using it in a 2-dimensional space. We can also use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose but, as mentioned, it is highly dependent on data type and also the domain that classification is done For it.
					
					dist((x, y), (a, b)) = √(x - a)² + (y - b)²
					Example - age
						dist(age1, age2)	 = √(age1 - age2)²
					Example - age and Income
						dist((age1,Income1), (age2,Income2)  =  √(age1 - age2)²  + (Income1 - Income2)²

	"how to select the correct K" 
		K in k-nearest neighbors, is the number of nearest neighbors to examine. It is supposed to be specified by the user. So, how do we choose the right K? Assume that we want to find the class of the customer noted as question mark on the chart. What happens if we choose a very low value of K, let’s say, k=1? The first nearest point would be Blue, which is class 1. This would be a bad prediction, since more of the points around it are Magenta, or class 4. In fact, since its nearest neighbor is Blue, we can say that we captured the noise in the data, or we chose one of the points that was an anomaly in the data. 

		OVER-FITTING - A low value of K causes a highly complex model as well, which might result in over-fitting of the model. It means the prediction process is not generalized enough to be used for out-of-sample cases. Out-of-sample data is data that is outside of the dataset used to train the model. In other words, it cannot be trusted to be used for prediction of unknown samples. It’s important to remember that over-fitting is bad, as we want a general model that works for any data, not just the data used for training. 
		UNDER-FITTING - Now, on the opposite side of the spectrum, if we choose a very high value of K, such as K=20, then the model becomes overly generalized. 

		So, how we can find the best value for K? The general solution is to reserve a part of your data for testing the accuracy of the model. Once you’ve done so, choose k =1, and then use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is best for your model. For example, in our case, k=4 will give us the best accuracy.	


Nearest neighbors analysis can also be used to compute values For a "continuous target".
	In this situation, the average or median target value of the nearest neighbors is used to obtain the predicted value for the new case.
	For example, assume that you are predicting the price of a home based on its feature set, such as number of rooms, square footage, the year it was built, and so on. You can easily find the three nearest neighbor houses, of course not only based on distance, but also based on all the attributes, and then predict the price of the house, as the "median" of neighbors.

EVALUATION METRICS
Jaccard index
					|y∩ŷ|			   |y∩ŷ|
	j(y , ŷ)   =   -------   =   ------------------					   ( 0 ....... to ........ 1) higher the better					
					|y∪ŷ|		  |y| + |ŷ| - |y∩ŷ|



F1-score - Confusion Matrix 
		Precision = Tp / (Tp + FP)
		Recall 	  = Tp / (Tp + FN)
		F1-score  = 2 * (Precision * Recall) / (Precision + Recall)   ( 0 ....... to ........ 1) higher the better


Log Loss :   		y log(ŷ) + (1-y) log(1- ŷ)
		 : -1/n  Σ [y log(ŷ) + (1-y) log(1- ŷ)]
		 															 ( 0 ....... to ........ 1)  lower the better






LAB: KNN
from sklearn.neighbors import KNeighborsClassifier

k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
>>>
	KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=None, n_neighbors=4, p=2,
           weights='uniform')

yhat = neigh.predict(X_test)
yhat[0:5]
>>>
	array([1, 1, 3, 2, 4])

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
>>>
	Train set Accuracy:  0.5475
	Test set Accuracy:  0.32

# We can calucalte the accuracy of KNN for different Ks.
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc
>>> array([0.3  , 0.29 , 0.315, 0.32 , 0.315, 0.31 , 0.335, 0.325, 0.34 ])

#Plot model accuracy for Different number of Neighbors
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

\ACCURACY 			= tp + tn / (Total)



print(classification_report(y_test, y_pred))
>>>
	              precision    recall  	  f1-score   support

           1       0.37      	0.51      0.43        51
           2       0.34      	0.34      0.34        44
           3       0.33      	0.33      0.33        54
           4       0.29      	0.18      0.22        51

   micro avg       0.34      	0.34      0.34        200
   macro avg       0.33      	0.34      0.33        200
weighted avg       0.33      	0.34      0.33        200


print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
>>>
	The best accuracy was with 0.34 with k= 9


plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=True)
plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=False)

**************************************************************************************
INTRO TO DECISION TREES
BUILDING DECISION TREES
- Decision trees are built using recursive partitioning to classify the data.
- The algorithm chooses the most predictive feature to split the data on.

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT.PNG"

\Steps:
1. Choose an attribute(feature/column) from your data set.
2. Calculate the significance of attribute in splitting of data.
3. Split data based on the value of the best attribute
4. Go to Step 1
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_steps.PNG"




\Entropy:
Entropy = - p(A)log(p(A)) - p(B)log(p(B))
The lower the entropy, the less uniform the distribution and the purer the node.
Entropy = 0 -> Pure node
Entropy = 1 -> uniform distribution of data
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_1.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_2.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_3.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_4.PNG"

\Information Gain:
Information Gain = Entropy before Split - Weighted Entropy after Split
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_IG.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_IG_1.PNG"

LAB: DECISION TREES
	Age	Sex	BP		Cholesterol	Na_to_K	Drug
0	23	F	HIGH	HIGH		25.355	drugY
1	47	M	LOW		HIGH		13.093	drugC
2	47	M	LOW		HIGH		10.114	drugC
3	28	F	NORMAL	HIGH		7.798	drugX
4	61	F	LOW		HIGH		18.043	drugY


Unfortunately, Sklearn Decision Trees do not handle categorical variables. But still we can convert these features to numerical values. pandas.get_dummies() Convert categorical variable into dummy/indicator variables.
X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values
X[0:5]
>>>
	array([[23, 'F', 'HIGH', 'HIGH', 25.355],
	       [47, 'M', 'LOW', 'HIGH', 13.093],
	       [47, 'M', 'LOW', 'HIGH', 10.113999999999999],
	       [28, 'F', 'NORMAL', 'HIGH', 7.797999999999999],
	       [61, 'F', 'LOW', 'HIGH', 18.043]], dtype=object)

TO

from sklearn import preprocessing
le_sex = preprocessing.LabelEncoder()
le_sex.fit(['F','M'])
X[:,1] = le_sex.transform(X[:,1]) 


le_BP = preprocessing.LabelEncoder()
le_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])
X[:,2] = le_BP.transform(X[:,2])


le_Chol = preprocessing.LabelEncoder()
le_Chol.fit([ 'NORMAL', 'HIGH'])
X[:,3] = le_Chol.transform(X[:,3]) 

X[0:5]

OR

X[:,1] = le_sex.fit_transform(X[:,1])
X[:,2] = le_BP.fit_transform(X[:,2])
X[:,3] = le_Chol.fit_transform(X[:,3])

X[0:5]
>>>
	array([[23, 0, 0, 0, 25.355],
	       [47, 1, 1, 0, 13.093],
	       [47, 1, 1, 0, 10.113999999999999],
	       [28, 0, 2, 0, 7.797999999999999],
	       [61, 0, 1, 0, 18.043]], dtype=object)


drugTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
>>>
	DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')


drugTree.fit(X_trainset,y_trainset)

predTree = drugTree.predict(X_testset)

print (predTree [0:5])
print (y_testset [0:5].values)
>>>
	['drugY' 'drugX' 'drugX' 'drugX' 'drugX']
	['drugY' 'drugX' 'drugX' 'drugX' 'drugX']

from sklearn import metrics
import matplotlib.pyplot as plt
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_testset, predTree))
OR
#calculate accuracy score without sklearn
x = np.array([y_testset== predTree]).sum()
x/len(y_testset)
>>>
0.98333333333


plot_confusion_matrix(y_testset,predTree)
# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
[[ 7  0  0  0  0]
 [ 0  5  0  0  0]
 [ 0  0  5  0  0]
 [ 0  0  0 20  1]
 [ 0  0  0  0 22]]

 Normalized confusion matrix
[[1.         0.         0.         0.         0.        ]
 [0.         1.         0.         0.         0.        ]
 [0.         0.         1.         0.         0.        ]
 [0.         0.         0.         0.95238095 0.04761905]
 [0.         0.         0.         0.         1.        ]]


 #Visualizing Decision Tree
from sklearn.externals.six import StringIO
import pydotplus
import matplotlib.image as mpimg
from sklearn import tree
%matplotlib inline 

dot_data = StringIO()
filename = "drugtree.png"
featureNames = my_data.columns[0:5]
targetNames = my_data["Drug"].unique().tolist()
out=tree.export_graphviz(drugTree,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_trainset), filled=True,  special_characters=True,rotate=False)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png(filename)
img = mpimg.imread(filename)
plt.figure(figsize=(100, 200))
plt.imshow(img,interpolation='nearest')
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_export_graphviz_code.png"



**************************************************************************************
INTRO TO LOGISTIC REGRESSION
1. What is  logistic regression?
2. What kind of problems solved?
3. Which situations we should use Logistic regression?

1. What is  logistic regression?
	Logistic regression is a statistical and machine learning technique for classifying records of a dataset, based on the values of the input fields.
	Let’s say we have a telecommunication dataset that we’d would like to analyze, in order	 to understand which customers might leave us next month.	 This is historical customer data where each row represents one customer.
	You’ll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group.
	INDEPENDENT VARIABLE : 
		In logistic regression, independent variables should be continuous; if categorical, they should be dummy or indicator-coded. This means we have to transform them to some continuous value.
	DEPENDENT VARIABLE : 
		logistic regression can be used for both binary classification and multiclass classification

2. What kind of problems solved?
	a. To predict the "probability" of a person having a heart attack within a specified time period, based on our knowledge of the person's age, sex, and body mass index.
	b. To predict the chance of mortality in an injured patient, or to "predict" whether a patient has a given disease, such as diabetes, based on observed characteristics of that patient, such as weight, height, blood pressure, and results of various blood tests, and so on.
	c. In a marketing context, we can use it to predict the "likelihood" of a customer purchasing a product or halting a subscription, as we’ve done in our churn example.
	d. To predict the "probability" of failure of a given process, system, or product.
	e. To predict the "likelihood" of a homeowner defaulting on a mortgage.

	\Notice that in all of these examples, not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.

3. Which situations we should use Logistic regression?
	Four situations in which Logistic regression is a good candidate:
		a. When the "target field in your data is categorical", or specifically, is binary, such as 0/1, yes/no, churn or no churn, positive/negative, and so on.
		
		b. When you "need the probability" of your prediction, for example, if you want to know what the probability is, of a customer buying a product. Logistic regression returns a probability score between 0 and 1 for a given sample of data. In fact, logistic regressing predicts the probability of that sample, and we map the cases to a discrete class based on that probability.
		
		c. If your data is "linearly separable".
				The decision boundary of logistic regression is a line or a plane or a hyper-plane. A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features (and are not applying any polynomial processing), we can obtain an inequality like θ_0+ θ_1 x_1+ θ_2 x_2 > 0, which is a half-plane, easily plottable.
				Please note that in using logistic regression, we can also achieve a complex decision boundary using polynomial processing as well, which is out of scope here.

		d. When you need to "understand the IMPACT" of a feature.
				You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.
				That is, after finding the optimum parameters, a feature x with the weight θ_1 close to 0, has a smaller effect on the prediction, than features with large absolute values of θ_1. Indeed, it allows us to understand the impact an independent variable has on the dependent variable while controlling other independent variables.

					ŷ    = 		P(y = 1|X)
				P(y=0|X) = 1 -  P(y = 1|X)



LOGISTIC VS LINEAR REGRESSION
Steps:
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/LogR_steps.PNG"

Choosing θ :
	Gradient Descent

Stopping Iteration :
	Satisfactory Accuracy Score.


LAB: LOGISTIC REGRESSION
While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

The objective of Logistic Regression algorithm, is to find the best parameters θ, for ℎ_θ(𝑥) = 𝜎({θ^TX}), in such a way that the model best predicts the class of each case.



# write your code here
parameters = {'C':[0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}
from sklearn.model_selection import GridSearchCV
#LR_1 = LogisticRegression(C=0.1, solver='liblinear').fit(X_train,y_train)
LR_1 = LogisticRegression()
cv = GridSearchCV(LR_1,parameters, cv = 5)
cv.fit(X_train,y_train)
print_results(cv)

print(log_loss(y_test, cv.best_estimator_.predict_proba(X_test)))
print (classification_report(y_test, cv.best_estimator_.predict(X_test)))
>>>

BEST PARAMS: {'C': 0.1, 'solver': 'liblinear'}

LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)
                                params  mean_test_score
7    {'C': 0.1, 'solver': 'liblinear'}          0.76875
2   {'C': 0.01, 'solver': 'liblinear'}          0.76250
10     {'C': 1, 'solver': 'newton-cg'}          0.75000
14          {'C': 1, 'solver': 'saga'}          0.75000
5    {'C': 0.1, 'solver': 'newton-cg'}          0.75000


0.731 (+/-0.018) for {'C': 0.01, 'solver': 'newton-cg'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'lbfgs'}
0.762 (+/-0.191) for {'C': 0.01, 'solver': 'liblinear'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'sag'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'saga'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'newton-cg'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'lbfgs'}
0.769 (+/-0.156) for {'C': 0.1, 'solver': 'liblinear'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'sag'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'saga'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'newton-cg'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 1, 'solver': 'liblinear'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'sag'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'saga'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'newton-cg'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'liblinear'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'sag'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'saga'}
0.5739445725558303
              precision    recall  f1-score   support

           0       0.69      1.00      0.82        25
           1       1.00      0.27      0.42        15

   micro avg       0.72      0.72      0.73        40
   macro avg       0.85      0.63      0.62        40
weighted avg       0.81      0.72      0.67        40

**************************************************************************************
SUPPORT VECTOR MACHINE
Imagine that you’ve obtained a dataset containing characteristics of thousands of human cell samples extracted from patients who were believed to be at risk of developing cancer. Analysis of the original data showed that many of the characteristics differed significantly between benign and malignant samples. You can use the values of these cell characteristics in samples from other patients to give an early indication of whether a new sample might be benign or malignant. You can use support vector machine, or SVM, as a classifier, to train your model to understand patterns within the data, that might show benign or malignant cells. Once the model has been trained, it can be used to predict your new or unknown cell with rather high accuracy. 

DEFINITION:
	A Support Vector Machine is a supervised algorithm that can classify cases by finding a "separator". 
	SVM works by first, 
		- mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. 
		- Then, a separator is estimated For the data. 

	The data should be transformed in such a way that a separator could be drawn as a hyperplane. For example, consider the following figure, which shows the distribution of a small set of cells, only based on their Unit Size and Clump thickness. As you can see, the data points fall into two different categories. It represents a linearly, non-separable, dataset. The two categories can be separated with a curve, but not a line. That is, it represents a linearly, non-separable dataset, which is the case For most real-world datasets. 
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/SVM_NonLinearSeperation.PNG"
		We can transfer this data to a higher dimensional space … For example, mapping it to a 3-dimensional space. After the transformation, the boundary between the two categories can be defined by a hyperplane. As we are now in 3-dimensional space, the separator is shown as a plane. This plane can be used to classify new or unknown cases. Therefore, the SVM algorithm outputs an optimal hyperplane that categorizes new examples. 
		"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/SVM_LinearSeperation_HigherDimension.PNG"

CHALLENGING QUESTIONS: 
		1. How do we transform data in such a way that a separator could be drawn as a hyperplane? and 
		2. How can we find the best/optimized hyperplane separator after transformation? 

		1. How do we transform data in such a way that a separator could be drawn as a hyperplane?
			 For the sake of simplicity, imagine that our dataset is 1-dimensional data, this means, we have only one feature x. As you can see, it is not linearly separable. So, what we can do here? Well, we can transform it into a 2-dimensional space. For example, your can increase the dimension of data by mapping x into a new space using a function, with outputs x and x-squared. Now, the data is linearly separable, right? Notice that, as we are in a two dimensional space, the hyperplane is a line dividing a plane into two parts where each class lays on either side. Now we can use this line to classify new cases. 
			 "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/SVM_DataTransformation.PNG"
			 Basically, mapping data into a higher dimensional space is called "KERNELLING". The mathematical function used For the transformation is known as the kernel function, and can be of different types, such as: Linear, Polynomial, Radial basis function (or RBF), and Sigmoid. Each of these functions has its own characteristics, its pros and cons, and its equation, but the good news is that you don’t need to know them, as most of them are already implemented in libraries of data science programming languages. Also, as there`s no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results. 

		2. How can we find the best/optimized hyperplane separator after transformation? 
			Basically, SVMs are based on the idea of" finding a hyperplane that best divides a dataset into two classes", as shown here. As we’re in a 2-dimensional space, you can think of the hyperplane as a line that linearly separates the blue points from the red points. 
			 - One reasonable choice as the best hyperplane is the one that represents the largest separation, or "MARGIN", between the two classes. So, the goal is to choose a hyperplane with as big a margin as possible. 
			 	Examples closest to the hyperplane are support vectors. 
			 	It is intuitive that only support vectors matter For achieving our goal; and thus, other training examples can be ignored. 
			 	We try to find the hyperplane in such a way that it has the maximum distance to support vectors. Please note, that the hyperplane and boundary decision lines have their own equations. So, finding the optimized hyperplane can be formalized using an equation which involves quite a bit more math, so I’m not going to go through it here, in detail. That said, the hyperplane is learned from training data using an optimization procedure that maximizes the margin; and like many other problems, this optimization problem can also be solved by gradient descent, which is out of scope of this video. Therefore, the output of the algorithm is the values ‘w’ and ‘b’ For the line. You can make classifications using this estimated line. It is enough to plug in input values into the line equation, then, you can calculate whether an unknown point is above or below the line. If the equation returns a value greater than 0, then the point belongs to the first class, which is above the line, and vice versa. 

ADVANTAGES :
	- they’re accurate in high dimensional spaces; and, 
	- they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient. 
DISADVANTAGES : 
	- The disadvantages of support vector machines include the fact that the algorithm is prone For over-fitting, if the number of features is much greater than the number of samples. (Fat-Short)
	- Also, SVMs do not directly provide probability estimates, which are desirable in most classification problems. 
	- And finally, SVMs are not very efficient computationally, if your dataset is very big, such as when you have more than one thousand rows.


"IN WHICH SITUATION SHOULD I USE SVM?"	 
	- SVM is good For image analysis tasks, such as image classification and handwritten digit recognition. 
	- Also SVM is very effective in text-mining tasks, particularly due to its effectiveness in dealing with high-dimensional data. For example, it is used For detecting spam, text category assignment, and sentiment analysis. 
	- Another application of SVM is in Gene Expression data classification, again, because of its power in high dimensional data classification. 
	- SVM can also be used For other types of machine learning problems, such as regression, outlier detection, and clustering. 



LAB: SUPPORT VECTOR MACHINES
SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.

cell_df = pd.read_csv("cell_samples.csv")
>>>
	ID			Clump	UnifSize	UnifShape	MargAdh	SingEpiSize	BareNuc	BlandChrom	NormNucl	Mit		Class
0	1000025		5		1			1			1		2			1		3			1			1		2
1	1002945		5		4			4			5		7			10		3			2			1		2
2	1015425		3		1			1			1		2			2		3			1			1		2
3	1016277		6		8			8			1		3			4		3			7			1		2
4	1017023		4		1			1			3		2			1		3			1			1		2


cell_df.dtypes
>>>
ID              int64
Clump           int64
UnifSize        int64
UnifShape       int64
MargAdh         int64
SingEpiSize     int64
BareNuc        object
BlandChrom      int64
NormNucl        int64
Mit             int64
Class           int64
dtype: object

\It looks like the BareNuc column includes some values that are not numerical. We can drop those rows:

cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()]
cell_df['BareNuc'] = cell_df['BareNuc'].astype('int')
cell_df.dtypes
>>>
ID             int64
Clump          int64
UnifSize       int64
UnifShape      int64
MargAdh        int64
SingEpiSize    int64
BareNuc        int64
BlandChrom     int64
NormNucl       int64
Mit            int64
Class          int64
dtype: object

feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]
X = np.asarray(feature_df)
>>>
array([[ 5,  1,  1,  1,  2,  1,  3,  1,  1],
       [ 5,  4,  4,  5,  7, 10,  3,  2,  1],
       [ 3,  1,  1,  1,  2,  2,  3,  1,  1],
       [ 6,  8,  8,  1,  3,  4,  3,  7,  1],
       [ 4,  1,  1,  3,  2,  1,  3,  1,  1]])

cell_df['Class'] = cell_df['Class'].astype('int')
y = np.asarray(cell_df['Class'])
>>>
array([2, 2, 2, 2, 2])

1.Linear
2.Polynomial
3.Radial basis function (RBF)
4.Sigmoid


from sklearn import svm
clf = svm.SVC(kernel='rbf')
clf.fit(X_train, y_train) 
>>>
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)

yhat = clf.predict(X_test)
>>>
array([2, 4, 2, 4, 2])

def plot_confusion_matrix_3(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4])
# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix_3(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')
>>>
Confusion matrix, without normalization
[[85  5]
 [ 0 47]]



np.set_printoptions(precision=2)
print (classification_report(y_test, yhat))
>>>
              precision    recall  f1-score   support

           2       1.00      0.94      0.97        90
           4       0.90      1.00      0.95        47

   micro avg       0.96      0.96      0.96       137
   macro avg       0.95      0.97      0.96       137
weighted avg       0.97      0.96     "0.96"      137



from sklearn.metrics import f1_score
f1_score(y_test, yhat, average='weighted') 
>>>
0.9639038982104676


from sklearn.metrics import jaccard_similarity_score
jaccard_similarity_score(y_test, yhat)
>>>
0.9635036496350365




# write your code here
def print_results(results):
    print('BEST PARAMS: {}\n'.format(results.best_params_))
    print(results.best_estimator_)
    df = pd.DataFrame(results.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]
    print(df[['params','mean_test_score']].head())
    means = results.cv_results_['mean_test_score']
    stds = results.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, results.cv_results_['params']):
        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))

# write your code here
parameters = {'kernel':['linear', 'poly', 'rbf', 'sigmoid' ]}
from sklearn.model_selection import GridSearchCV
svm_1 = svm.SVC()
cv = GridSearchCV(svm_1,parameters, cv = 5)
cv.fit(X_train, y_train)
print_results(cv)
>>>
BEST PARAMS: {'kernel': 'linear'}

SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='linear', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
                  params  mean_test_score
0   {'kernel': 'linear'}         0.970696
2      {'kernel': 'rbf'}         0.957875
1     {'kernel': 'poly'}         0.937729
3  {'kernel': 'sigmoid'}         0.349817
0.971 (+/-0.047) for {'kernel': 'linear'}
0.938 (+/-0.022) for {'kernel': 'poly'}
0.958 (+/-0.05) for {'kernel': 'rbf'}
0.35 (+/-0.055) for {'kernel': 'sigmoid'}




print (classification_report(y_test, cv.best_estimator_.predict(X_test)))
>>>
              precision    recall  f1-score   support

           2       1.00      0.94      0.97        90
           4       0.90      1.00      0.95        47

   micro avg       0.96      0.96      0.96       137
   macro avg       0.95      0.97      0.96       137
weighted avg       0.97      0.96      0.96       137
**************************************************************************************
GRADED REVIEW QUESTIONS