INTRO TO CLASSIFICATION

K-NEAREST NEIGHBORS
EVALUATION METRICS
LAB: KNN

INTRO TO DECISION TREES
BUILDING DECISION TREES
LAB: DECISION TREES

INTRO TO LOGISTIC REGRESSION
LOGISTIC VS LINEAR REGRESSION
LAB: LOGISTIC REGRESSION

SUPPORT VECTOR MACHINE
LAB: SUPPORT VECTOR MACHINES

GRADED REVIEW QUESTIONS

**************************************************************************************
INTRO TO CLASSIFICATION
In Machine Learning, classification is a supervised learning approach, which can be thought of as a means of categorizing or "classifying" some unknown items into a discrete set of "classes."
The goal of a loan default predictor is to use existing loan default data, which is information about the customers (such as age, income, education, etc.), to build a classifier, pass a new customer or potential future defaulter to the model, and then label it (i.e. the data points) as "Defaulter" or "Not Defaulter", or for example, 0 or 1. This is how a classifier predicts an unlabeled test case.
We can also build classifier models For both binary classification and multi-class classification.

Business Use-cases : 
	To predict the category to which a customer belongs;
	For Churn detection, where we predict whether a customer switches to another provider or brand; 
	Or to predict whether or not a customer responds to a particular advertising campaign.

Applications :
	email filtering, 
	speech recognition, 
	handwriting recognition, 
	bio-metric identification, 
	document classification, and much more.

Algorithms : 
	1. Decision Trees, ✔
	2. Naïve Bayes, 
	3. Linear Discriminant Analysis, 
	4. K-nearest neighbor,✔
	5. Logistic regression, ✔
	6. Neural Networks, 
	7. Support Vector Machines.✔

**************************************************************************************
K-NEAREST NEIGHBORS
The example focuses on using demographic data, such as region, age, and marital status, to predict usage patterns. The target field, called custcat, has four possible values that correspond to the four customer groups, as follows: Basic Service, E-Service, Plus Service, and Total Service. Our objective is to build a classifier, for example using the rows 0 to 7, to predict the class of row 8.

"How can we find the class of this customer?	"
	Can we find one of the closest cases and assign the same class label to our new customer?	Can we also say that the class of our new customer is most probably group 4 (i.e. total	service) because its nearest neighbor is also of "class 4"?	Yes, we can. In fact, it is the first-nearest neighbor.

"To what extent can we trust our judgment, which is based on the first nearest neighbor?"
	It might be a poor judgment, especially if the first nearest neighbor is a very specific case, or an "outlier", correct? Now, let’s look at our scatter plot again. Rather than choose the first nearest neighbor, what" if we chose the five nearest neighbors", and did a "majority vote" among them to define the class of our new customer? In this case, we’d see that three out of five nearest neighbors tell us to go For "class 3", which is ”Plus service.”
	In this case, the value of K in the k-nearest neighbors algorithm is 5.

K-NEAREST NEIGHBORS. 
	The k-nearest-neighbors algorithm is a classification algorithm that takes a bunch of labelled points and uses them to learn how to label other points. 
	This algorithm classifies cases based on their similarity to other cases. In k-nearest neighbors, data points that are near each other are said to be "neighbors". 
	K-nearest neighbors is based on this paradigm: "“Similar cases with the same class labels are near each other.”" Thus, the distance between two cases is a measure of their dissimilarity. 
	There are different ways to calculate the similarity, or conversely, the distance or dissimilarity of two data points. For example, this can be done using  "Euclidian distance".

K-NN Algorithm working :
	1. Pick a value For K.
	2. Calculate the distance of unknown case from all cases (holdout from each of the cases in the dataset).
	3. Search for the K observations in the training data that are ‘nearest’ to the measurements of the unknown data point.
	4. Predict the response of the unknown data point using the "most popular response" value from the K nearest neighbors.

There are two parts in this algorithm that might be a bit confusing.
	First, "how to select the correct K";
	Second, "how to compute the similarity between cases", for example, among customers?

	"how to compute the similarity between cases"
		Assume that we have two customers, customer 1 and customer 2. And, For a moment, assume that these 2 customers have only one feature, Age. We can easily use a specific type of Minkowski distance to calculate the distance of these 2 customers. It is indeed, the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 to power of 2, which is 4. What about if we have more than one feature, for example Age and Income? If we have income and age For each customer, we can still use the same formula, but this time, we’re using it in a 2-dimensional space. We can also use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose but, as mentioned, it is highly dependent on data type and also the domain that classification is done For it.
					
					dist((x, y), (a, b)) = √(x - a)² + (y - b)²
					Example - age
						dist(age1, age2)	 = √(age1 - age2)²
					Example - age and Income
						dist((age1,Income1), (age2,Income2)  =  √(age1 - age2)²  + (Income1 - Income2)²

	"how to select the correct K" 
		K in k-nearest neighbors, is the number of nearest neighbors to examine. It is supposed to be specified by the user. So, how do we choose the right K? Assume that we want to find the class of the customer noted as question mark on the chart. What happens if we choose a very low value of K, let’s say, k=1? The first nearest point would be Blue, which is class 1. This would be a bad prediction, since more of the points around it are Magenta, or class 4. In fact, since its nearest neighbor is Blue, we can say that we captured the noise in the data, or we chose one of the points that was an anomaly in the data. 

		OVER-FITTING - A low value of K causes a highly complex model as well, which might result in over-fitting of the model. It means the prediction process is not generalized enough to be used for out-of-sample cases. Out-of-sample data is data that is outside of the dataset used to train the model. In other words, it cannot be trusted to be used for prediction of unknown samples. It’s important to remember that over-fitting is bad, as we want a general model that works for any data, not just the data used for training. 
		UNDER-FITTING - Now, on the opposite side of the spectrum, if we choose a very high value of K, such as K=20, then the model becomes overly generalized. 

		So, how we can find the best value for K? The general solution is to reserve a part of your data for testing the accuracy of the model. Once you’ve done so, choose k =1, and then use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is best for your model. For example, in our case, k=4 will give us the best accuracy.	


Nearest neighbors analysis can also be used to compute values For a "continuous target".
	In this situation, the average or median target value of the nearest neighbors is used to obtain the predicted value for the new case.
	For example, assume that you are predicting the price of a home based on its feature set, such as number of rooms, square footage, the year it was built, and so on. You can easily find the three nearest neighbor houses, of course not only based on distance, but also based on all the attributes, and then predict the price of the house, as the "median" of neighbors.

EVALUATION METRICS
Jaccard index
					|y∩ŷ|			   |y∩ŷ|
	j(y , ŷ)   =   -------   =   ------------------					   ( 0 ....... to ........ 1) higher the better					
					|y∪ŷ|		  |y| + |ŷ| - |y∩ŷ|



F1-score - Confusion Matrix 
		Precision = Tp / (Tp + FP)
		Recall 	  = Tp / (Tp + FN)
		F1-score  = 2 * (Precision * Recall) / (Precision + Recall)   ( 0 ....... to ........ 1) higher the better


Log Loss :   		y log(ŷ) + (1-y) log(1- ŷ)
		 : -1/n  Σ [y log(ŷ) + (1-y) log(1- ŷ)]
		 															 ( 0 ....... to ........ 1)  lower the better






LAB: KNN
from sklearn.neighbors import KNeighborsClassifier

k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
>>>
	KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=None, n_neighbors=4, p=2,
           weights='uniform')

yhat = neigh.predict(X_test)
yhat[0:5]
>>>
	array([1, 1, 3, 2, 4])

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
>>>
	Train set Accuracy:  0.5475
	Test set Accuracy:  0.32

# We can calucalte the accuracy of KNN for different Ks.
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc
>>> array([0.3  , 0.29 , 0.315, 0.32 , 0.315, 0.31 , 0.335, 0.325, 0.34 ])

#Plot model accuracy for Different number of Neighbors
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

\ACCURACY 			= tp + tn / (Total)



print(classification_report(y_test, y_pred))
>>>
	              precision    recall  	  f1-score   support

           1       0.37      	0.51      0.43        51
           2       0.34      	0.34      0.34        44
           3       0.33      	0.33      0.33        54
           4       0.29      	0.18      0.22        51

   micro avg       0.34      	0.34      0.34        200
   macro avg       0.33      	0.34      0.33        200
weighted avg       0.33      	0.34      0.33        200


print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
>>>
	The best accuracy was with 0.34 with k= 9


plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=True)
plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=False)

**************************************************************************************
INTRO TO DECISION TREES
BUILDING DECISION TREES
- Decision trees are built using recursive partitioning to classify the data.
- The algorithm chooses the most predictive feature to split the data on.

"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT.PNG"

\Steps:
1. Choose an attribute(feature/column) from your data set.
2. Calculate the significance of attribute in splitting of data.
3. Split data based on the value of the best attribute
4. Go to Step 1
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_steps.PNG"




\Entropy:
Entropy = - p(A)log(p(A)) - p(B)log(p(B))
The lower the entropy, the less uniform the distribution and the purer the node.
Entropy = 0 -> Pure node
Entropy = 1 -> uniform distribution of data
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_1.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_2.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_3.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_Entropy_4.PNG"

\Information Gain:
Information Gain = Entropy before Split - Weighted Entropy after Split
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_IG.PNG"
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/DT_IG_1.PNG"

LAB: DECISION TREES
	Age	Sex	BP		Cholesterol	Na_to_K	Drug
0	23	F	HIGH	HIGH		25.355	drugY
1	47	M	LOW		HIGH		13.093	drugC
2	47	M	LOW		HIGH		10.114	drugC
3	28	F	NORMAL	HIGH		7.798	drugX
4	61	F	LOW		HIGH		18.043	drugY


Unfortunately, Sklearn Decision Trees do not handle categorical variables. But still we can convert these features to numerical values. pandas.get_dummies() Convert categorical variable into dummy/indicator variables.
X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values
X[0:5]
>>>
array([[23, 'F', 'HIGH', 'HIGH', 25.355],
       [47, 'M', 'LOW', 'HIGH', 13.093],
       [47, 'M', 'LOW', 'HIGH', 10.113999999999999],
       [28, 'F', 'NORMAL', 'HIGH', 7.797999999999999],
       [61, 'F', 'LOW', 'HIGH', 18.043]], dtype=object)

TO

from sklearn import preprocessing
le_sex = preprocessing.LabelEncoder()
le_sex.fit(['F','M'])
X[:,1] = le_sex.transform(X[:,1]) 


le_BP = preprocessing.LabelEncoder()
le_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])
X[:,2] = le_BP.transform(X[:,2])


le_Chol = preprocessing.LabelEncoder()
le_Chol.fit([ 'NORMAL', 'HIGH'])
X[:,3] = le_Chol.transform(X[:,3]) 

X[0:5]

OR

X[:,1] = le_sex.fit_transform(X[:,1])
X[:,2] = le_BP.fit_transform(X[:,2])
X[:,3] = le_Chol.fit_transform(X[:,3])

X[0:5]
>>>
array([[23, 0, 0, 0, 25.355],
       [47, 1, 1, 0, 13.093],
       [47, 1, 1, 0, 10.113999999999999],
       [28, 0, 2, 0, 7.797999999999999],
       [61, 0, 1, 0, 18.043]], dtype=object)


drugTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
>>>
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')


drugTree.fit(X_trainset,y_trainset)

predTree = drugTree.predict(X_testset)

print (predTree [0:5])
print (y_testset [0:5].values)
['drugY' 'drugX' 'drugX' 'drugX' 'drugX']
['drugY' 'drugX' 'drugX' 'drugX' 'drugX']

from sklearn import metrics
import matplotlib.pyplot as plt
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_testset, predTree))
OR
#calculate accuracy score without sklearn
x = np.array([y_testset== predTree]).sum()
x/len(y_testset)
>>>
0.98333333333

**************************************************************************************
INTRO TO LOGISTIC REGRESSION
LOGISTIC VS LINEAR REGRESSION
LAB: LOGISTIC REGRESSION
**************************************************************************************
SUPPORT VECTOR MACHINE
LAB: SUPPORT VECTOR MACHINES
**************************************************************************************
GRADED REVIEW QUESTIONS