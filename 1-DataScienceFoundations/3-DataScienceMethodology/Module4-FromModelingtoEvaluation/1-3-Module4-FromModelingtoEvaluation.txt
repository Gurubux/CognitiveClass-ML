Module 4: From Modeling to Evaluation
	Modeling
	Evaluation

------------------------------------------------------------------
MODELING
Data Modelling focuses on developing models that are either descriptive or predictive.
An example of a "descriptive model" might examine things like: if a person did this, then they`re likely to prefer that.
A "predictive model" tries to yield yes/no, or stop/go type outcomes. These models are based on the analytic approach that was taken, either statistically driven or machine learning driven.
The data scientist will use a training set for predictive modelling.

The success of data compilation, preparation and modelling, depends on the understanding of the problem at hand, and the appropriate analytical approach being taken.
The data supports the answering of the question, and like the quality of the ingredients in cooking, sets the stage for the outcome. Constant refinement, adjustments and tweaking are necessary within each step to ensure the outcome is one that is solid.

In John Rollins` descriptive Data Science Methodology, the framework is geared to do 3 things: 
	First, understand the question at hand. 
	Second, select an analytic approach or method to solve the problem, and
	third, obtain, understand, prepare, and model the data.

The end goal is to move the data scientist to a point where a data model can be built to answer the question.

In this stage of the methodology, model evaluation, deployment, and feedback loops ensure that the answer is near and relevant.
------------------------------------------------------------------
EVALUATION
Model evaluation can have two main phases.
	The first is the diagnostic measures phase, which is used to ensure the model is working as intended.
		If the model is a predictive model, a decision tree can be used to evaluate if the answer the model can output, is aligned to the initial design. It can be used to see where there are areas that require adjustments.
		If the model is a descriptive model, one in which relationships are being assessed, then a testing set with known outcomes can be applied, and the model can be refined as needed.
	The second phase of evaluation that may be used is statistical significance testing.
		This type of evaluation can be applied to the model to ensure that the data is being properly handled and interpreted within the model.
	This is designed to avoid unnecessary second guessing when the answer is revealed.
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/1-DataScienceFoundations/3-DataScienceMethodology/Module4-FromModelingtoEvaluation/DataEvaluation-WhenAndHowToAdjustModel.PNG"


Let`s look at one way to find the optimal model through a diagnostic measure based on tuning one of the parameters in model building.
Specifically we`ll see how to tune the relative cost of misclassifying yes and no outcomes.
As shown in this table "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/1-DataScienceFoundations/3-DataScienceMethodology/Module4-FromModelingtoEvaluation/CaseStudy-RelativeCost-TPR-FPR.PNG", four models were built with four different relative misclassification costs.
As we see, each value of this model-building parameter increases the true-positive rate, or sensitivity, of the accuracy in predicting yes, at the expense of lower accuracy in predicting no, that is, an increasing false-positive rate.
The question then becomes, which model is best based on tuning this parameter?
For budgetary reasons, the risk-reducing intervention could not be applied to most or all congestive heart failure patients, many of whom would not have been readmitted anyway.
On the other hand, the intervention would not be as effective in improving patient care as it should be, with not enough high-risk congestive heart failure patients targeted.

SO, HOW DO WE DETERMINE WHICH MODEL WAS OPTIMAL? "ROC Curve : True-Positive Rate vs False-Positive Rate"

As you can see on this slide "https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/1-DataScienceFoundations/3-DataScienceMethodology/Module4-FromModelingtoEvaluation/CostStudy-ROC_Curve.PNG", the optimal model is the one giving the maximum separation between the blue ROC curve relative to the red base line.
We can see that model 3, with a relative misclassification cost of 4-to-1, is the best of the 4 models.

#And just in case you were wondering, ROC stands for receiver operating characteristic curve, which was first developed during World War II to detect enemy aircraft on radar. It has since been used in many other fields as well. Today it is commonly used in machine learning and data mining.

The ROC curve is a useful diagnostic tool in determining the "OPTIMAL CLASSIFICATION MODEL."
This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied. In this case, the criterion is a relative misclassification cost.
By plotting the true-positive rate against the false-positive rate for different values of the relative misclassification cost, the ROC curve helped in selecting the optimal model.



------------------------------------------------------------------
LABS
In [2]:
import pandas as pd # import library to read data into dataframe
pd.set_option("display.max_columns", None)
import numpy as np # import numpy library
import re # import library for regular expression
import random # library for random number generation
In [3]:
recipes = pd.read_csv("https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DS0103EN/labs/data/recipes.csv")

print("Data read into dataframe!") # takes about 30 seconds
Data read into dataframe!
In [4]:
# fix name of the column displaying the cuisine
column_names = recipes.columns.values
column_names[0] = "cuisine"
recipes.columns = column_names

# convert cuisine names to lower case
recipes["cuisine"] = recipes["cuisine"].str.lower()

# make the cuisine names consistent
recipes.loc[recipes["cuisine"] == "austria", "cuisine"] = "austrian"
recipes.loc[recipes["cuisine"] == "belgium", "cuisine"] = "belgian"
recipes.loc[recipes["cuisine"] == "china", "cuisine"] = "chinese"
recipes.loc[recipes["cuisine"] == "canada", "cuisine"] = "canadian"
recipes.loc[recipes["cuisine"] == "netherlands", "cuisine"] = "dutch"
recipes.loc[recipes["cuisine"] == "france", "cuisine"] = "french"
recipes.loc[recipes["cuisine"] == "germany", "cuisine"] = "german"
recipes.loc[recipes["cuisine"] == "india", "cuisine"] = "indian"
recipes.loc[recipes["cuisine"] == "indonesia", "cuisine"] = "indonesian"
recipes.loc[recipes["cuisine"] == "iran", "cuisine"] = "iranian"
recipes.loc[recipes["cuisine"] == "italy", "cuisine"] = "italian"
recipes.loc[recipes["cuisine"] == "japan", "cuisine"] = "japanese"
recipes.loc[recipes["cuisine"] == "israel", "cuisine"] = "jewish"
recipes.loc[recipes["cuisine"] == "korea", "cuisine"] = "korean"
recipes.loc[recipes["cuisine"] == "lebanon", "cuisine"] = "lebanese"
recipes.loc[recipes["cuisine"] == "malaysia", "cuisine"] = "malaysian"
recipes.loc[recipes["cuisine"] == "mexico", "cuisine"] = "mexican"
recipes.loc[recipes["cuisine"] == "pakistan", "cuisine"] = "pakistani"
recipes.loc[recipes["cuisine"] == "philippines", "cuisine"] = "philippine"
recipes.loc[recipes["cuisine"] == "scandinavia", "cuisine"] = "scandinavian"
recipes.loc[recipes["cuisine"] == "spain", "cuisine"] = "spanish_portuguese"
recipes.loc[recipes["cuisine"] == "portugal", "cuisine"] = "spanish_portuguese"
recipes.loc[recipes["cuisine"] == "switzerland", "cuisine"] = "swiss"
recipes.loc[recipes["cuisine"] == "thailand", "cuisine"] = "thai"
recipes.loc[recipes["cuisine"] == "turkey", "cuisine"] = "turkish"
recipes.loc[recipes["cuisine"] == "vietnam", "cuisine"] = "vietnamese"
recipes.loc[recipes["cuisine"] == "uk-and-ireland", "cuisine"] = "uk-and-irish"
recipes.loc[recipes["cuisine"] == "irish", "cuisine"] = "uk-and-irish"


# remove data for cuisines with < 50 recipes:
recipes_counts = recipes["cuisine"].value_counts()
cuisines_indices = recipes_counts > 50

cuisines_to_keep = list(np.array(recipes_counts.index.values)[np.array(cuisines_indices)])
recipes = recipes.loc[recipes["cuisine"].isin(cuisines_to_keep)]

# convert all Yes's to 1's and the No's to 0's
recipes = recipes.replace(to_replace="Yes", value=1)
recipes = recipes.replace(to_replace="No", value=0)
In [5]:
# import decision trees scikit-learn libraries
%matplotlib inline
from sklearn import tree
from sklearn.metrics import accuracy_score, confusion_matrix

import matplotlib.pyplot as plt

!conda install python-graphviz --yes
import graphviz

from sklearn.tree import export_graphviz

import itertools



In [6]:
recipes.head()


In [7]:
# select subset of cuisines
asian_indian_recipes = recipes[recipes.cuisine.isin(["korean", "japanese", "chinese", "thai", "indian"])]
cuisines = asian_indian_recipes["cuisine"]
ingredients = asian_indian_recipes.iloc[:,1:]

bamboo_tree = tree.DecisionTreeClassifier(max_depth=3)
bamboo_tree.fit(ingredients, cuisines)

print("Decision tree model saved to bamboo_tree!")



In [8]:
export_graphviz(bamboo_tree,
                feature_names=list(ingredients.columns.values),
                out_file="bamboo_tree.dot",
                class_names=np.unique(cuisines),
                filled=True,
                node_ids=True,
                special_characters=True,
                impurity=False,
                label="all",
                leaves_parallel=False)

with open("bamboo_tree.dot") as bamboo_tree_image:
    bamboo_tree_graph = bamboo_tree_image.read()
graphviz.Source(bamboo_tree_graph)
\"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/1-DataScienceFoundations/3-DataScienceMethodology/Module4-FromModelingtoEvaluation/DT_1.png"

In [9]:
bamboo = recipes[recipes.cuisine.isin(["korean", "japanese", "chinese", "thai", "indian"])]
In [10]:
bamboo["cuisine"].value_counts()
Out[10]:
korean      799
indian      598
chinese     442
japanese    320
thai        289
Name: cuisine, dtype: int64


In [11]:
# set sample size
sample_n = 30
In [12]:
# take 30 recipes from each cuisine
random.seed(1234) # set random seed
bamboo_test = bamboo.groupby("cuisine", group_keys=False).apply(lambda x: x.sample(sample_n))

bamboo_test_ingredients = bamboo_test.iloc[:,1:] # ingredients
bamboo_test_cuisines = bamboo_test["cuisine"] # corresponding cuisines or labels
In [13]:
# check that we have 30 recipes from each cuisine
bamboo_test["cuisine"].value_counts()
Out[13]:
thai        30
chinese     30
indian      30
japanese    30
korean      30
Name: cuisine, dtype: int64
In [14]:
bamboo_test_index = bamboo.index.isin(bamboo_test.index)
bamboo_train = bamboo[~bamboo_test_index]

bamboo_train_ingredients = bamboo_train.iloc[:,1:] # ingredients
bamboo_train_cuisines = bamboo_train["cuisine"] # corresponding cuisines or labels
In [15]:
bamboo_train["cuisine"].value_counts()
Out[15]:
korean      769
indian      568
chinese     412
japanese    290
thai        259
Name: cuisine, dtype: int64
In [16]:
bamboo_train_tree = tree.DecisionTreeClassifier(max_depth=15)
bamboo_train_tree.fit(bamboo_train_ingredients, bamboo_train_cuisines)

print("Decision tree model saved to bamboo_train_tree!")
Decision tree model saved to bamboo_train_tree!
In [17]:
export_graphviz(bamboo_train_tree,
                feature_names=list(bamboo_train_ingredients.columns.values),
                out_file="bamboo_train_tree.dot",
                class_names=np.unique(bamboo_train_cuisines),
                filled=True,
                node_ids=True,
                special_characters=True,
                impurity=False,
                label="all",
                leaves_parallel=False)

with open("bamboo_train_tree.dot") as bamboo_train_tree_image:
    bamboo_train_tree_graph = bamboo_train_tree_image.read()
graphviz.Source(bamboo_train_tree_graph)
\"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/1-DataScienceFoundations/3-DataScienceMethodology/Module4-FromModelingtoEvaluation/DT_2.png"

-------------------------------------------------------------
GRADED ASSIGNMENT
A training set is used for predictive modeling.

Type I ERROR 	: "false positive" or "false hit".
Type II ERROR 	: "false negative" or "miss".


Model evaluation can include statistical significance testing.  
Model evaluation includes ensuring that the data are properly handled and interpreted.  
Model evaluation includes ensuring the model is designed as intended.  
Model evaluation includes ensuring that the model is working as intended.